{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import csv\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torchvision.transforms import transforms\n",
    "from torchvision.datasets.utils import download_and_extract_archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_SDEV = [0.229, 0.224, 0.225]\n",
    "\n",
    "\n",
    "# Return all the image paths in a folder\n",
    "get_image_file = lambda image_dir: [os.path.join(image_dir, f) for f in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, f))]\n",
    "get_image = lambda folder, index: Image.open(folder[index])\n",
    "get_path = lambda x: os.path.join(os.getcwd(),'data', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CopyDetectPretrainDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 image_dir: str):\n",
    "        self.image_dir = image_dir\n",
    "        self.image_files = np.array([os.path.join(image_dir, f) \n",
    "                                     for f in os.listdir(image_dir) \n",
    "                                     if os.path.isfile(os.path.join(image_dir, f))])\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        image_id = self.image_files[index]\n",
    "        image = Image.open(os.path.join(self.image_dir, image_id))\n",
    "        return image, image_id\n",
    "\n",
    "class CopyDetectCollateFn(nn.Module):\n",
    "    def __init__(self,\n",
    "                 transform,\n",
    "                 augment: object,\n",
    "                 n_crops: Optional[int] = 1):\n",
    "        super().__init__()\n",
    "        self.transform  = transform\n",
    "        self.augment = augment\n",
    "        self.n_crops = n_crops\n",
    "\n",
    "    def forward(self, batch):\n",
    "        batch_size = len(batch)\n",
    "        indices = np.arange(batch_size)\n",
    "        # Transform image in batch and give a dimension for batching\n",
    "        imgs, ids = batch\n",
    "        ref_imgs = list(map(lambda x: self.transform(x).unsqueeze_(dim = 0), batch))\n",
    "        \n",
    "        ref_imgs_list, aug_imgs_list, ref_ids_list, aug_ids_list = [], [], [], []\n",
    "        \n",
    "        for _ in range(self.n_crops):\n",
    "            rand_bool = np.random.uniform(size = batch_size) < 0.5\n",
    "            rand_indices = np.random.randint(0, batch_size, size = batch_size)\n",
    "            aug_indices = np.where(rand_bool, indices, rand_indices)\n",
    "            aug_imgs = list(map(lambda i: batch[i], aug_indices.tolist()))\n",
    "            aug_imgs = list(map(lambda x: self.transform(self.augment(x)).unsqueeze_(dim = 0), aug_imgs))\n",
    "            aug_ids = ids[aug_indices.tolist()]\n",
    "            \n",
    "            ref_imgs_list.extend(ref_imgs), aug_imgs_list.extend(aug_imgs), ref_ids_list.extend(ids), aug_ids_list.extend(aug_ids)\n",
    "            \n",
    "        return torch.vstack(ref_imgs_list), torch.vstack(aug_imgs_list), torch.hstack(ref_ids_list), torch.hstack(aug_ids_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CopyDetectCollateFn(nn.Module):\n",
    "    def __init__(self,\n",
    "                 transform,\n",
    "                 augment: object,\n",
    "                 n_crops: Optional[int] = 1):\n",
    "        super().__init__()\n",
    "        self.transform  = transform\n",
    "        self.augment = augment\n",
    "        self.n_crops = n_crops\n",
    "\n",
    "    def forward(self, batch):\n",
    "        batch_size = len(batch)\n",
    "        indices = np.arange(batch_size)\n",
    "        # Transform image in batch and give a dimension for batching\n",
    "        imgs, ids = batch\n",
    "        ref_imgs = list(map(lambda x: self.transform(x).unsqueeze_(dim = 0), batch))\n",
    "        \n",
    "        ref_imgs_list, aug_imgs_list, ref_ids_list, aug_ids_list = [], [], [], []\n",
    "        \n",
    "        for _ in range(self.n_crops):\n",
    "            rand_bool = np.random.uniform(size = batch_size) < 0.5\n",
    "            rand_indices = np.random.randint(0, batch_size, size = batch_size)\n",
    "            aug_indices = np.where(rand_bool, indices, rand_indices)\n",
    "            aug_imgs = list(map(lambda i: batch[i], aug_indices.tolist()))\n",
    "            aug_imgs = list(map(lambda x: self.transform(self.augment(x)).unsqueeze_(dim = 0), aug_imgs))\n",
    "            aug_ids = ids[aug_indices.tolist()]\n",
    "            \n",
    "            ref_imgs_list.extend(ref_imgs), aug_imgs_list.extend(aug_imgs), ref_ids_list.extend(ids), aug_ids_list.extend(aug_ids)\n",
    "            \n",
    "        return torch.vstack(ref_imgs_list), torch.vstack(aug_imgs_list), torch.hstack(ref_ids_list), torch.hstack(aug_ids_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datamodules.components.augmentation import Augment\n",
    "\n",
    "transform =  transforms.Compose([transforms.ToTensor(),\n",
    "                                 transforms.Resize((224, 224)),\n",
    "                                 transforms.Normalize(IMAGENET_MEAN, IMAGENET_SDEV)])\n",
    "\n",
    "augment = Augment(overlay_image_dir = get_path('train/'),\n",
    "                  n_upper = 2,\n",
    "                  n_lower = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = CopyDetectCollateFn(tranform = transform,\n",
    "                                 augment = augment,\n",
    "                                 n_crops = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CopyDetectPretrainDataset(image_dir = get_path('dev_queries/'))\n",
    "\n",
    "train_dataloader = DataLoader(dataset       = train_dataset,\n",
    "                              batch_size    = 16,\n",
    "                              num_workers   = 8,\n",
    "                              pin_memory    = True,\n",
    "                              collate_fn    = collate_fn,\n",
    "                              shuffle       = True,\n",
    "                              drop_last     = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTModel\n",
    "from src.models.components.layers import CopyDetectEmbedding, NormalizedFeatures, SimImagePred, ContrastiveProj\n",
    "\n",
    "pretrained_arch = 'google/vit-base-patch16-224'\n",
    "pretrained_model = ViTModel.from_pretrained(pretrained_arch)\n",
    "encoder = pretrained_model.encoder\n",
    "\n",
    "                    \n",
    "patch_size = pretrained_model.config.patch_size\n",
    "                \n",
    "# Instantiate embedding, we use the pretrained ViT cls and position embedding\n",
    "embedding = CopyDetectEmbedding(config      = pretrained_model.config,\n",
    "                                vit_cls     = pretrained_model.embeddings.cls_token,\n",
    "                                pos_emb     = pretrained_model.embeddings.position_embeddings)\n",
    "        \n",
    "# Normalized features\n",
    "normfeats = NormalizedFeatures(hidden_dim   = pretrained_model.config.hidden_size,\n",
    "                               layer_norm_eps = pretrained_model.config.layer_norm_eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrastiveproj = ContrastiveProj(embedding_dim = 786,\n",
    "                                  hidden_dim = 1024,\n",
    "                                  projected_dim = 512)\n",
    "\n",
    "contrastiveproj = nn.Sequential(embedding, encoder, normfeats, contrastiveproj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "    ref_img, query_img, ref_id, query_id = batch\n",
    "    label = torch.tensor(ref_id == query_id, dtype = torch.bool)\n",
    "    proj_r = contrastiveproj(ref_img[label])\n",
    "    proj_q = contrastiveproj(query_img[label])\n",
    "    \n",
    "    loss = contrastiveproj(proj_r, proj_q)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
