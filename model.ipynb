{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import einops\n",
    "from PIL import Image\n",
    "from typing import Any, List, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from torchmetrics import MaxMetric\n",
    "from torchmetrics.classification.accuracy import Accuracy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from transformers import ViTModel\n",
    "\n",
    "from src.models.components.layers import CopyDetectEmbedding, NormalizedFeatures, SimImagePred, ContrastiveProj\n",
    "from src.datamodules.copydetect_datamodule import CopyDetectDataModule\n",
    "from src.datamodules.components.augmentation import Augment\n",
    "from src.utils.nt_xent_loss import NTXentLoss\n",
    "\n",
    "\n",
    "get_path = lambda x: os.path.join(os.getcwd(),'data', x)\n",
    "\n",
    "augment = Augment(overlay_image_dir = get_path('train/'),\n",
    "                  n_upper = 2,\n",
    "                  n_lower = 1)\n",
    "\n",
    "ntxentloss = NTXentLoss(temperature = 0.9, eps = 1e-5)\n",
    "\n",
    "device = torch.device('cuda:4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/vit-base-patch16-224 were not used when initializing ViTModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "class CopyDetectModule(LightningModule):\n",
    "    def __init__(self,\n",
    "                 pretrained_arch: str,          # Pretrained ViT architecture\n",
    "                 ntxentloss: object,            # Contrastive loss\n",
    "                 hidden_dim: int = 2048,        # Contrastive projection size of hidden layer\n",
    "                 projected_dim: int = 512,      # Contrastive projection size of projection head \n",
    "                 beta1: int = 1,                # Similar image BCE loss multiplier\n",
    "                 beta2: int = 1,                # Contrastive loss multiplier\n",
    "                 lr: float = 0.001,\n",
    "                 weight_decay: float = 0.0005):               \n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(logger = False)\n",
    "         \n",
    "        # Instantiate ViT encoder from pretrained model\n",
    "        pretrained_model = ViTModel.from_pretrained(pretrained_arch)\n",
    "        encoder = pretrained_model.encoder\n",
    "        self.patch_size = pretrained_model.config.patch_size\n",
    "                \n",
    "        # Instantiate embedding, we use the pretrained ViT cls and position embedding\n",
    "        embedding = CopyDetectEmbedding(config = pretrained_model.config,\n",
    "                                        vit_cls = pretrained_model.embeddings.cls_token,\n",
    "                                        pos_emb = pretrained_model.embeddings.position_embeddings)\n",
    "        \n",
    "        # Normalized features\n",
    "        normfeats = NormalizedFeatures(hidden_dim = pretrained_model.config.hidden_size,\n",
    "                                       layer_norm_eps = pretrained_model.config.layer_norm_eps)\n",
    "        # Feature Vector Extractor\n",
    "        self.feature_extractor = nn.Sequential(embedding, encoder, normfeats)\n",
    "        \n",
    "        # Instantiate SimImagePredictor\n",
    "        simimagepred = SimImagePred(embedding_dim = pretrained_model.config.hidden_size)\n",
    "        self.embedding = embedding\n",
    "        self.simimagepred = nn.Sequential(encoder, normfeats, simimagepred)\n",
    "\n",
    "        # Instantiate ContrastiveProjection\n",
    "        contrastiveproj = ContrastiveProj(embedding_dim = pretrained_model.config.hidden_size,\n",
    "                                          hidden_dim = hidden_dim,\n",
    "                                          projected_dim = projected_dim)\n",
    "        self.contrastiveproj = nn.Sequential(embedding, encoder, normfeats, contrastiveproj)\n",
    "        \n",
    "        # Contrastive loss \n",
    "        self.contrastive_loss = ntxentloss\n",
    "        \n",
    "        # Binary cross entropy loss for similar image pair\n",
    "        self.bce_loss = torch.nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        # Model accuracy in detecting modified copy\n",
    "        self.train_acc, self.val_acc = Accuracy(), Accuracy()   \n",
    "          \n",
    "        # For logging best validation accuracy\n",
    "        self.val_acc_best = MaxMetric()\n",
    "\n",
    "    def feature_extract(self, batch: Any) -> torch.Tensor:\n",
    "        # To extract feature vector\n",
    "        img_r, img_id = batch\n",
    "        encoding = self.feature_extractor(img_r)\n",
    "        batch_size, num_ch, H, W, = img_r.size()\n",
    "        #dim = encoding.size(2) # batch_size, seq_len, dim \n",
    "        h, w = int(H/self.patch_size), int(W/self.patch_size)\n",
    "        cls, feats = encoding[:,0,:], encoding[:,1:,:] # Get the cls token and all the images features\n",
    "            \n",
    "        #feats = feats.reshape(batch_size, h, w, dim).clamp(min = 1e-6).permute(0,3,1,2)\n",
    "        feats = einops.rearrange(feats, 'b (h w) d -> b d h w', h = h, w = w).clamp(min = 1e-6)\n",
    "        # GeM Pooling\n",
    "        feats = F.avg_pool2d(feats.pow(4), (h,w)).pow(1./4)\n",
    "        feats = einops.rearrange(feats, 'b d () () -> b d')\n",
    "        # Concatenate cls tokens with image patches to give local and global views of image\n",
    "        feature_vector = torch.cat((cls, feats), dim = 1)\n",
    "\n",
    "        return feature_vector, img_id\n",
    "        \n",
    "    def predict_copy(self, batch):\n",
    "        # For copy detection \n",
    "        img_r, img_q = batch\n",
    "        embedding_rq = self.embedding(img_r, img_q)\n",
    "        logits = self.simimagepred(embedding_rq)\n",
    "        preds = torch.argmax(logits, dim = 1)\n",
    "        return preds\n",
    "\n",
    "    def step(self,\n",
    "             img_r: List[torch.Tensor],\n",
    "             img_q: List[torch.Tensor],\n",
    "             label: List[torch.Tensor]):\n",
    "        \n",
    "        # img_r, img_q to SimImagePredictor\n",
    "        embedding_rq = self.embedding(img_r, img_q) ## nn sequential don't take multiple input\n",
    "        logits = self.simimagepred(embedding_rq)\n",
    "        # Calculate binary cross entropy loss of similar image pair\n",
    "        simimage_loss = self.bce_loss(logits, label.unsqueeze(dim = 1))\n",
    "        # Predictions\n",
    "        preds = torch.argmax(logits, dim = 1)\n",
    "        \n",
    "        # Get positive indices\n",
    "        pos_indices = label.bool()\n",
    "        # Forward positive indices of img_r and img_q to ContrastiveProjection\n",
    "        proj_r = self.contrastiveproj(img_r[pos_indices])\n",
    "        proj_q = self.contrastiveproj(img_q[pos_indices])\n",
    "\n",
    "        # Calculate contrastive loss between un-augmented img_r and augmented positive pair of img_q\n",
    "        contrastive_loss = self.contrastive_loss(proj_r, proj_q)\n",
    "        \n",
    "        # Weighted sum of bce and contrastive loss\n",
    "        total_loss = self.hparams.beta1 * simimage_loss + self.hparams.beta2 * contrastive_loss\n",
    "        \n",
    "        return {'simimage': simimage_loss, 'contrastive': contrastive_loss, 'total': total_loss}, preds\n",
    "\n",
    "    def training_step(self, batch: Any, batch_idx: int):\n",
    "        img_r, img_q, label = batch\n",
    "        img_r, img_q, label = torch.vstack(img_r), torch.vstack(img_q), torch.hstack(label)\n",
    "\n",
    "        losses, preds = self.step(img_r, img_q, label)\n",
    "        \n",
    "        # Log train metrics\n",
    "        acc = self.train_acc(preds, label.int())\n",
    "        self.log(\"train/total_loss\", losses['total'], on_step = True, on_epoch = True, prog_bar = False)\n",
    "        self.log(\"train/simimage_loss\", losses['simimage'], on_step = True, on_epoch = True, prog_bar = False)\n",
    "        self.log(\"train/contrastive_loss\", losses['contrastive'], on_step = True, on_epoch = True, prog_bar = False)\n",
    "        self.log(\"train/acc\", acc, on_step = True, on_epoch = True, prog_bar = True)\n",
    "\n",
    "        return losses['total']\n",
    "\n",
    "    def validation_step(self, batch: Any, batch_idx: int):\n",
    "        img_r, img_q, label = batch\n",
    "        losses, preds = self.step(img_r, img_q, label)\n",
    "\n",
    "        # Log val metrics\n",
    "        acc = self.val_acc(preds, label.int())\n",
    "        self.log(\"val/total_loss\", losses['total'], on_step = True, on_epoch = True, prog_bar = False)\n",
    "        self.log(\"val/simimage_loss\", losses['simimage'], on_step = True, on_epoch = True, prog_bar = False)\n",
    "        self.log(\"val/contrastive_loss\", losses['contrastive'], on_step = True, on_epoch = True, prog_bar = False)\n",
    "        self.log(\"val/acc\", acc, on_step = True, on_epoch = True, prog_bar = True)\n",
    "\n",
    "        return losses['total']\n",
    "\n",
    "    def validation_epoch_end(self, outputs: Any):\n",
    "        acc = self.val_acc.compute()  # get val accuracy from current epoch\n",
    "        self.val_acc_best.update(acc)\n",
    "        self.log(\"val/acc_best\", self.val_acc_best.compute(), on_epoch = True, prog_bar = True)\n",
    "        \n",
    "    def test_step(self, batch: Any, batch_idx: int):\n",
    "        feats = self.feature_extract(batch) # Get feat\n",
    "        \n",
    "        return feats\n",
    "    \n",
    "    def test_epoch_end(self, test_step_outputs: Any):\n",
    "        all_feats, all_ids = [], []\n",
    "        for step_output in test_step_outputs:\n",
    "            all_feats.append(step_output[0])\n",
    "            all_ids.extend(step_output[1])\n",
    "            \n",
    "        all_feats = torch.vstack(all_feats)\n",
    "        self.test_results = (all_feats, all_ids)\n",
    "        \n",
    "        return all_feats\n",
    "    \n",
    "    def predict_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0):\n",
    "        self.test_results = None\n",
    "        score = self.predict_copy(batch)\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        # Reset metrics at the end of every epoch\n",
    "        self.train_acc.reset()\n",
    "        self.val_acc.reset()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(params = self.parameters(),\n",
    "                                lr = self.hparams.lr,\n",
    "                                weight_decay = self.hparams.weight_decay)\n",
    "        \n",
    "pretrained_arch = 'google/vit-base-patch16-224'\n",
    "\n",
    "model = CopyDetectModule(pretrained_arch, ntxentloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = CopyDetectDataModule(train_dir = get_path('train/'),\n",
    "                                  references_dir = get_path('references/'),\n",
    "                                  dev_queries_dir = get_path('dev_queries/'),\n",
    "                                  final_queries_dir = get_path('final_queries/'),\n",
    "                                  augment = augment,\n",
    "                                  dev_validation_set = get_path('dev_validation_set.csv'),\n",
    "                                  batch_size = 16,\n",
    "                                  pin_memory = True,\n",
    "                                  num_workers = 10,\n",
    "                                  n_crops = 2)\n",
    "\n",
    "datamodule.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(accelerator = 'gpu', devices = [4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86ffb988e2e14c5ebc49463aadddf633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trainer.test(model = model, dataloaders = datamodule.references_dataloader())\n",
    "r, r_id = model.test_results\n",
    "r = r.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bd21c914f3c48a7838036b83fea1a2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trainer.test(model = model, dataloaders = datamodule.final_queries_dataloader())\n",
    "q, q_id = model.test_results\n",
    "q = q.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_SDEV = [0.229, 0.224, 0.225]\n",
    "\n",
    "transform =  transforms.Compose([transforms.ToTensor(),\n",
    "                                 transforms.Resize((224, 224)),\n",
    "                                 transforms.Normalize(IMAGENET_MEAN, IMAGENET_SDEV)])\n",
    "\n",
    "get_image = lambda img_dir, img: Image.open(os.path.join(img_dir, img))\n",
    "get_image_file = lambda image_dir: [os.path.join(image_dir, f) for f in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, f))]\n",
    "\n",
    "class CopyDetectPredDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 predictions: list,\n",
    "                 references_dir: str,\n",
    "                 final_queries_dir: str):\n",
    "        self.predictions = predictions\n",
    "        self.references_dir = references_dir\n",
    "        self.final_queries_dir = final_queries_dir\n",
    "        self.transform =  transforms.Compose([transforms.ToTensor(),\n",
    "                                              transforms.Resize((224, 224)),\n",
    "                                              transforms.Normalize(IMAGENET_MEAN, IMAGENET_SDEV)])\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.predictions)\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        final_queries_id, references_id = self.predictions[index][0], self.predictions[index][1]\n",
    "        \n",
    "        reference_image = get_image(self.references_dir, references_id)\n",
    "        final_queries_image = get_image(self.final_queries_dir, final_queries_id)\n",
    "        \n",
    "        return self.transform(reference_image), self.transform(final_queries_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import search_with_capped_res\n",
    "\n",
    "lims, dis, ids = search_with_capped_res(q, r, 1000)\n",
    "\n",
    "predictions_list = []\n",
    "for i in range(100):\n",
    "    for j in range(lims[i], lims[i+1]):\n",
    "        predictions_list.append([q_id[i], r_id[ids[j]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "copydetectpred = CopyDetectPredDataset(predictions = predictions_list,\n",
    "                                       references_dir = get_path('references'),\n",
    "                                       final_queries_dir = get_path('final_queries/'))\n",
    "\n",
    "\n",
    "copydetect_dataloader = DataLoader(dataset = copydetectpred,\n",
    "                                   batch_size = 16,\n",
    "                                   num_workers = 8,\n",
    "                                   pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ba89607162e46ba9031ed4a69bdb6fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    }
   ],
   "source": [
    "p = trainer.predict(model = model, dataloaders = copydetect_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.hstack(p).cpu().numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/leejiahe/copydetection/model.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bidsd-1.d2.comp.nus.edu.sg/home/leejiahe/copydetection/model.ipynb#ch0000031vscode-remote?line=0'>1</a>\u001b[0m ref_feats, ref_ids \u001b[39m=\u001b[39m [], []\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bidsd-1.d2.comp.nus.edu.sg/home/leejiahe/copydetection/model.ipynb#ch0000031vscode-remote?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m datamodule\u001b[39m.\u001b[39mreferences_dataloader():\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bidsd-1.d2.comp.nus.edu.sg/home/leejiahe/copydetection/model.ipynb#ch0000031vscode-remote?line=3'>4</a>\u001b[0m     feats, img_id \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfeature_extract(batch)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bidsd-1.d2.comp.nus.edu.sg/home/leejiahe/copydetection/model.ipynb#ch0000031vscode-remote?line=4'>5</a>\u001b[0m     ref_feats\u001b[39m.\u001b[39mappend(feats)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bidsd-1.d2.comp.nus.edu.sg/home/leejiahe/copydetection/model.ipynb#ch0000031vscode-remote?line=5'>6</a>\u001b[0m     ref_ids\u001b[39m.\u001b[39mextend(img_id)\n",
      "\u001b[1;32m/home/leejiahe/copydetection/model.ipynb Cell 2'\u001b[0m in \u001b[0;36mCopyDetectModule.feature_extract\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bidsd-1.d2.comp.nus.edu.sg/home/leejiahe/copydetection/model.ipynb#ch0000017vscode-remote?line=52'>53</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfeature_extract\u001b[39m(\u001b[39mself\u001b[39m, batch: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bidsd-1.d2.comp.nus.edu.sg/home/leejiahe/copydetection/model.ipynb#ch0000017vscode-remote?line=53'>54</a>\u001b[0m     \u001b[39m# To extract feature vector\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bidsd-1.d2.comp.nus.edu.sg/home/leejiahe/copydetection/model.ipynb#ch0000017vscode-remote?line=54'>55</a>\u001b[0m     img_r, img_id \u001b[39m=\u001b[39m batch\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bidsd-1.d2.comp.nus.edu.sg/home/leejiahe/copydetection/model.ipynb#ch0000017vscode-remote?line=55'>56</a>\u001b[0m     encoding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeature_extractor(img_r)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bidsd-1.d2.comp.nus.edu.sg/home/leejiahe/copydetection/model.ipynb#ch0000017vscode-remote?line=56'>57</a>\u001b[0m     batch_size, num_ch, H, W, \u001b[39m=\u001b[39m img_r\u001b[39m.\u001b[39msize()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bidsd-1.d2.comp.nus.edu.sg/home/leejiahe/copydetection/model.ipynb#ch0000017vscode-remote?line=57'>58</a>\u001b[0m     \u001b[39m#dim = encoding.size(2) # batch_size, seq_len, dim \u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/container.py?line=136'>137</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/container.py?line=137'>138</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/container.py?line=138'>139</a>\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/container.py?line=139'>140</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/cama/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py:377\u001b[0m, in \u001b[0;36mViTEncoder.forward\u001b[0;34m(self, hidden_states, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py?line=370'>371</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py?line=371'>372</a>\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py?line=372'>373</a>\u001b[0m         hidden_states,\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py?line=373'>374</a>\u001b[0m         layer_head_mask,\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py?line=374'>375</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py?line=375'>376</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py?line=376'>377</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(hidden_states, layer_head_mask, output_attentions)\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py?line=378'>379</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py?line=380'>381</a>\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/cama/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py:332\u001b[0m, in \u001b[0;36mViTLayer.forward\u001b[0;34m(self, hidden_states, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py?line=328'>329</a>\u001b[0m layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate(layer_output)\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py?line=330'>331</a>\u001b[0m \u001b[39m# second residual connection is done here\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py?line=331'>332</a>\u001b[0m layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput(layer_output, hidden_states)\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py?line=333'>334</a>\u001b[0m outputs \u001b[39m=\u001b[39m (layer_output,) \u001b[39m+\u001b[39m outputs\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py?line=335'>336</a>\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/cama/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py:294\u001b[0m, in \u001b[0;36mViTOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py?line=292'>293</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states, input_tensor):\n\u001b[0;32m--> <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py?line=293'>294</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdense(hidden_states)\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py?line=294'>295</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py?line=296'>297</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m hidden_states \u001b[39m+\u001b[39m input_tensor\n",
      "File \u001b[0;32m~/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/linear.py:96\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/linear.py?line=94'>95</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/linear.py?line=95'>96</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/functional.py:1847\u001b[0m, in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/functional.py?line=1844'>1845</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight):\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/functional.py?line=1845'>1846</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[39minput\u001b[39m, weight), \u001b[39minput\u001b[39m, weight, bias\u001b[39m=\u001b[39mbias)\n\u001b[0;32m-> <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/functional.py?line=1846'>1847</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, weight, bias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ref_feats, ref_ids = [], []\n",
    "\n",
    "for batch in datamodule.references_dataloader():\n",
    "    feats, img_id = model.feature_extract(batch)\n",
    "    ref_feats.append(feats)\n",
    "    ref_ids.extend(img_id)\n",
    "ref_feats = torch.vstack(ref_feats).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/leejiahe/copydetection/model.ipynb Cell 15'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bidsd-1.d2.comp.nus.edu.sg/home/leejiahe/copydetection/model.ipynb#ch0000080vscode-remote?line=0'>1</a>\u001b[0m model(img)\n",
      "File \u001b[0;32m~/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/cama/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py:629\u001b[0m, in \u001b[0;36mLightningModule.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py?line=617'>618</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py?line=618'>619</a>\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py?line=619'>620</a>\u001b[0m \u001b[39m    Same as :meth:`torch.nn.Module.forward()`.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py?line=620'>621</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py?line=626'>627</a>\u001b[0m \u001b[39m        Your model's output\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py?line=627'>628</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py?line=628'>629</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py:201\u001b[0m, in \u001b[0;36m_forward_unimplemented\u001b[0;34m(self, *input)\u001b[0m\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=189'>190</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_forward_unimplemented\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39minput\u001b[39m: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=190'>191</a>\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Defines the computation performed at every call.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=191'>192</a>\u001b[0m \n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=192'>193</a>\u001b[0m \u001b[39m    Should be overridden by all subclasses.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=198'>199</a>\u001b[0m \u001b[39m        registered hooks while the latter silently ignores them.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=199'>200</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=200'>201</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_feats, query_ids = [], []\n",
    "\n",
    "for batch in datamodule.final_queries_dataloader():\n",
    "    img, img_id = batch\n",
    "    feats = model(img.to(device))\n",
    "    query_feats.append(feats)\n",
    "    query_ids.extend(img_id)\n",
    "    \n",
    "query_feats = torch.vstack(query_feats).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import search_with_capped_res\n",
    "\n",
    "lims, dis, ids = search_with_capped_res(query_feats, ref_feats, 1000)\n",
    "\n",
    "predictions_list = []\n",
    "for i in range(100):\n",
    "    for j in range(lims[i], lims[i+1]):\n",
    "        predictions_list.append([query_ids[i], ref_ids[ids[j]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.test_results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for searches in copydetect_dataloader:\n",
    "    query_id, ref_id = searches\n",
    "    query_id, ref_id = query_id.to(device), ref_id.to(device)\n",
    "    score = model(ref_id, query_id)\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = torch.hstack(scores).detach().cpu().numpy()\n",
    "scores.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class CopyDetectPretrainDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 image_dir: str):\n",
    "        \n",
    "        self.image_files = np.array([os.path.join(image_dir, f) \n",
    "                                     for f in os.listdir(image_dir) \n",
    "                                     if os.path.isfile(os.path.join(image_dir, f))])\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "            return self.image_files[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CopyDetectPretrainDataset(image_dir = get_path('references/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datamodules.components.augmentation import Augment\n",
    "\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_SDEV = [0.229, 0.224, 0.225]\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Resize((224, 224)),\n",
    "                                transforms.Normalize(IMAGENET_MEAN, IMAGENET_SDEV)])\n",
    "\n",
    "\n",
    "\n",
    "augment = Augment(overlay_image_dir = get_path('references/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CopyDetectCollateFn(nn.Module):\n",
    "    def __init__(self,\n",
    "                 transform,\n",
    "                 image_dir: str,\n",
    "                 augment: object,\n",
    "                 n_crops: Optional[int] = 1):\n",
    "        super().__init__()\n",
    "        self.transform  = transform\n",
    "        self.augment = augment\n",
    "        self.n_crops = n_crops\n",
    "        self.image_files = np.array([os.path.join(image_dir, f) \n",
    "                                     for f in os.listdir(image_dir) \n",
    "                                     if os.path.isfile(os.path.join(image_dir, f))])\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        imgs = list(map(lambda x: self.transform(Image.open(x)).unsqueeze_(dim = 0), batch))\n",
    "        imgs_list, aug_list, label_list = [], [], []\n",
    "        for _ in range(self.n_crops):\n",
    "            aug_index = batch\n",
    "            aug_index = list(map(lambda x: random.choice(self.image_files) if random.random() > 0.5 else x, aug_index)) # Get random image file\n",
    "            aug_imgs = list(map(lambda x: self.transform(self.augment(Image.open(x))).unsqueeze_(dim = 0), aug_index))\n",
    "            \n",
    "            label = torch.tensor([i == j for i,j in zip(aug_index, batch)], dtype = torch.float) # label 1: modified copy: aug_index == index \n",
    "            #print(aug_index)\n",
    "            #print(batch)\n",
    "            #print(label)\n",
    "            imgs_list.extend(imgs)\n",
    "            aug_list.extend(aug_imgs)\n",
    "            label_list.extend(label)\n",
    "            \n",
    "        \n",
    "        imgs_list = torch.vstack(imgs_list)\n",
    "        aug_list = torch.vstack(aug_list)\n",
    "        label_list = torch.hstack(label_list)\n",
    "        print(aug_list.size())\n",
    "        #return imgs_list, aug_list, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "collate_fn = CopyDetectCollateFn(transform = transform,\n",
    "                                 image_dir = get_path('references/'),\n",
    "                                 augment = augment,\n",
    "                                 n_crops = 4)\n",
    "\n",
    "dataloader = DataLoader(dataset = train_dataset,\n",
    "                        collate_fn = collate_fn,\n",
    "                        batch_size = 8)\n",
    "\n",
    "next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ['/home/leejiahe/copydetection/data/references/R056915.jpg', '/home/leejiahe/copydetection/data/references/R565046.jpg', '/home/leejiahe/copydetection/data/references/R694596.jpg', '/home/leejiahe/copydetection/data/references/R971542.jpg', '/home/leejiahe/copydetection/data/references/R193894.jpg', '/home/leejiahe/copydetection/data/references/R933667.jpg', '/home/leejiahe/copydetection/data/references/R685573.jpg', '/home/leejiahe/copydetection/data/references/R664814.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = ['/home/leejiahe/copydetection/data/references/R056915.jpg', '/home/leejiahe/copydetection/data/references/R326673.jpg', '/home/leejiahe/copydetection/data/references/R694596.jpg', '/home/leejiahe/copydetection/data/references/R971542.jpg', '/home/leejiahe/copydetection/data/references/R193894.jpg', '/home/leejiahe/copydetection/data/references/R344586.jpg', '/home/leejiahe/copydetection/data/references/R685573.jpg', '/home/leejiahe/copydetection/data/references/R772966.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, False, True, True, True, False, True, False]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i == j for i, j in zip(a, b)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    ToTensor()\n",
       "    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=None)\n",
       "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[-0.6812, -1.1906, -0.7321,  ..., -0.7711, -0.9279, -1.1461],\n",
       "          [-1.0254, -1.4291, -0.5844,  ..., -0.8351, -1.1097, -1.2178],\n",
       "          [-1.1836, -1.3217, -0.2871,  ..., -0.8718, -1.1022, -1.0739],\n",
       "          ...,\n",
       "          [ 1.2013,  1.1047,  0.6806,  ...,  0.8459,  0.2968,  0.4611],\n",
       "          [ 1.0654,  1.3428,  1.1892,  ...,  0.7320,  0.7362,  1.0430],\n",
       "          [ 0.5898,  1.0979,  0.9440,  ...,  0.5567,  1.0771,  0.0623]],\n",
       " \n",
       "         [[-0.5853, -1.1219, -0.8590,  ..., -0.9564, -1.0817, -1.2699],\n",
       "          [-1.0414, -1.4669, -0.7923,  ..., -1.0219, -1.2676, -1.3431],\n",
       "          [-1.2428, -1.4419, -0.4898,  ..., -1.0595, -1.2568, -1.1991],\n",
       "          ...,\n",
       "          [ 1.1933,  1.0631,  0.6355,  ...,  0.7538,  0.2516,  0.3558],\n",
       "          [ 1.0611,  1.3097,  1.1527,  ...,  0.6327,  0.7070,  0.9507],\n",
       "          [ 0.5749,  1.0593,  0.9020,  ...,  0.4710,  1.0727, -0.0520]],\n",
       " \n",
       "         [[-0.8820, -1.3765, -1.0796,  ..., -0.8694, -0.9767, -1.1814],\n",
       "          [-1.1264, -1.5438, -0.8585,  ..., -0.9346, -1.1618, -1.2544],\n",
       "          [-1.2750, -1.4556, -0.5633,  ..., -0.9720, -1.1526, -1.1095],\n",
       "          ...,\n",
       "          [ 1.2624,  1.2046,  0.7801,  ...,  0.9189,  0.4394,  0.5516],\n",
       "          [ 1.1217,  1.4564,  1.3000,  ...,  0.7999,  0.8912,  1.1164],\n",
       "          [ 0.6377,  1.2071,  1.0505,  ...,  0.5865,  1.2040,  0.1182]]]),\n",
       " tensor([[[ 0.2820,  0.3994,  0.3542,  ..., -0.6758, -0.2758,  0.3004],\n",
       "          [ 0.4104,  0.4961,  0.3236,  ..., -0.5938, -0.2342,  0.2673],\n",
       "          [ 0.4728,  0.5364,  0.4484,  ..., -0.6244, -0.4348, -0.0213],\n",
       "          ...,\n",
       "          [-0.8653, -1.0060, -1.0721,  ...,  0.7591,  0.6404,  0.8288],\n",
       "          [-0.7883, -0.9938, -0.9375,  ...,  0.7529,  0.6392,  0.7933],\n",
       "          [-0.8335, -0.8739, -0.7063,  ...,  0.7627,  0.6563,  0.8373]],\n",
       " \n",
       "         [[-0.4226, -0.2325, -0.3138,  ..., -0.6877, -0.6139, -0.4038],\n",
       "          [-0.2850, -0.1337, -0.3450,  ..., -0.6877, -0.6176, -0.4663],\n",
       "          [-0.2100, -0.0749, -0.2175,  ..., -0.7027, -0.7377, -0.6302],\n",
       "          ...,\n",
       "          [-0.8777, -0.9515, -0.9840,  ...,  0.7829,  0.6616,  0.8542],\n",
       "          [-0.7990, -0.9565, -0.9040,  ...,  0.7654,  0.6491,  0.8067],\n",
       "          [-0.7927, -0.8865, -0.7152,  ...,  0.7692,  0.6604,  0.8454]],\n",
       " \n",
       "         [[-1.0525, -0.8633, -0.9965,  ..., -0.9044, -1.1421, -1.4696],\n",
       "          [-0.8633, -0.6952, -0.9927,  ..., -0.8807, -1.1247, -1.4671],\n",
       "          [-0.7886, -0.6367, -0.8658,  ..., -0.8608, -1.1409, -1.4546],\n",
       "          ...,\n",
       "          [-0.8259, -0.8819, -0.8446,  ...,  0.7228,  0.6020,  0.7938],\n",
       "          [-0.7126, -0.8869, -0.7886,  ...,  0.7390,  0.6232,  0.7801],\n",
       "          [-0.7238, -0.7998, -0.6292,  ...,  0.7614,  0.6531,  0.8373]]]),\n",
       " tensor([[[ 0.2254, -0.4667,  0.1252,  ...,  2.1975,  2.1975,  2.1975],\n",
       "          [-0.2071, -1.1708, -1.0786,  ...,  2.1975,  2.1975,  2.1975],\n",
       "          [-1.3816, -1.6516, -1.6390,  ...,  2.1975,  2.1975,  2.1975],\n",
       "          ...,\n",
       "          [-0.1706, -0.5581, -0.3038,  ..., -0.4804, -0.6261, -0.5393],\n",
       "          [-0.5584, -0.4662, -1.0063,  ..., -0.4640, -0.4216, -0.4856],\n",
       "          [-0.2998, -0.5492, -0.6014,  ..., -0.4630, -0.5036, -0.5931]],\n",
       " \n",
       "         [[ 0.3855, -0.3602, -0.0049,  ...,  2.4101,  2.3936,  2.3936],\n",
       "          [ 0.0371, -1.0512, -1.1843,  ...,  2.4111,  2.3936,  2.3936],\n",
       "          [-1.1873, -1.5702, -1.6830,  ...,  2.3936,  2.3936,  2.3936],\n",
       "          ...,\n",
       "          [-0.6552, -0.8377, -0.9109,  ..., -0.7998, -0.9325, -0.8445],\n",
       "          [-0.9697, -0.9280, -1.4423,  ..., -0.8526, -0.7980, -0.8675],\n",
       "          [-0.6475, -0.9221, -0.9055,  ..., -0.8604, -0.8659, -0.9005]],\n",
       " \n",
       "         [[ 0.2472, -0.8516, -0.1585,  ...,  2.2586,  2.2914,  2.2974],\n",
       "          [-0.5105, -1.6208, -1.4953,  ...,  2.2566,  2.2914,  2.2914],\n",
       "          [-1.6658, -1.8044, -1.8044,  ...,  2.2914,  2.3257,  2.3263],\n",
       "          ...,\n",
       "          [-1.2612, -1.4810, -1.4149,  ..., -1.2848, -1.3334, -1.2477],\n",
       "          [-1.5100, -1.5069, -1.7876,  ..., -1.1986, -1.1984, -1.3591],\n",
       "          [-1.2409, -1.5648, -1.4767,  ..., -1.2655, -1.2415, -1.3434]]]),\n",
       " tensor([[[ 1.2703,  1.4269,  1.4323,  ...,  2.2318,  2.2062,  2.2326],\n",
       "          [ 1.2482,  1.2202,  1.2055,  ...,  2.2375,  2.2208,  2.2270],\n",
       "          [ 1.0196,  0.9592,  1.0110,  ...,  2.2134,  2.2134,  2.2291],\n",
       "          ...,\n",
       "          [ 1.1915,  0.9684, -0.1401,  ..., -0.5736, -0.5308, -0.5951],\n",
       "          [ 0.0097,  0.6620,  0.4409,  ..., -0.6577, -0.3650, -0.5401],\n",
       "          [ 1.1670,  0.8642,  0.4200,  ...,  0.0316, -0.4319, -0.4986]],\n",
       " \n",
       "         [[ 1.4281,  1.6056,  1.6288,  ...,  1.6758,  1.6496,  1.6766],\n",
       "          [ 1.4693,  1.4232,  1.3969,  ...,  1.6816,  1.6645,  1.6709],\n",
       "          [ 1.2569,  1.1626,  1.1631,  ...,  1.6395,  1.6566,  1.6730],\n",
       "          ...,\n",
       "          [ 1.1387,  0.9131, -0.1690,  ..., -0.6120, -0.5007, -0.5139],\n",
       "          [-0.0932,  0.5889,  0.3889,  ..., -0.6712, -0.3487, -0.5277],\n",
       "          [ 1.2137,  0.9491,  0.5243,  ...,  0.0567, -0.4521, -0.5516]],\n",
       " \n",
       "         [[ 1.8531,  2.0298,  2.0703,  ...,  0.8797,  0.8360,  0.8456],\n",
       "          [ 1.8418,  1.8146,  1.8221,  ...,  0.8854,  0.8684,  0.8399],\n",
       "          [ 1.5768,  1.5017,  1.5519,  ...,  0.8958,  0.8619,  0.8769],\n",
       "          ...,\n",
       "          [ 1.0745,  0.8512, -0.2435,  ..., -0.6521, -0.6074, -0.6555],\n",
       "          [-0.1034,  0.5296,  0.3243,  ..., -0.7076, -0.4038, -0.5696],\n",
       "          [ 1.1517,  0.8746,  0.4003,  ...,  0.0601, -0.4545, -0.4838]]]),\n",
       " tensor([[[-1.5941, -1.4913, -1.5360,  ..., -0.6604, -0.6757, -0.7297],\n",
       "          [-1.5717, -1.5904, -1.6078,  ..., -0.3674, -0.3541, -0.2444],\n",
       "          [-1.5408, -1.5127, -1.5343,  ..., -0.1604, -0.1720, -0.2605],\n",
       "          ...,\n",
       "          [-1.6341, -1.6979, -1.6494,  ..., -1.1200, -1.1358, -1.0898],\n",
       "          [-1.2622, -1.3182, -1.2278,  ..., -1.0900, -1.1320, -1.0354],\n",
       "          [-0.5026, -0.4614, -0.4742,  ..., -1.0565, -1.0161, -1.0430]],\n",
       " \n",
       "         [[-1.6580, -1.5527, -1.6149,  ..., -1.1591, -1.0486, -1.0452],\n",
       "          [-1.5724, -1.5821, -1.6171,  ..., -1.0175, -0.9006, -0.7439],\n",
       "          [-1.5364, -1.5077, -1.5617,  ..., -0.8542, -0.7385, -0.8287],\n",
       "          ...,\n",
       "          [-1.7287, -1.7296, -1.7220,  ..., -1.2955, -1.3113, -1.2648],\n",
       "          [-1.4050, -1.3745, -1.3302,  ..., -1.2650, -1.3079, -1.2091],\n",
       "          [-0.6817, -0.6042, -0.6527,  ..., -1.2307, -1.1894, -1.2169]],\n",
       " \n",
       "         [[-1.5832, -1.5954, -1.5965,  ..., -1.2629, -1.2045, -1.2332],\n",
       "          [-1.5825, -1.6856, -1.6729,  ..., -1.2301, -1.1406, -1.0038],\n",
       "          [-1.3802, -1.3865, -1.4373,  ..., -1.1472, -1.0445, -1.0998],\n",
       "          ...,\n",
       "          [-1.6572, -1.7171, -1.6583,  ..., -1.3309, -1.3581, -1.3136],\n",
       "          [-1.3664, -1.4067, -1.3455,  ..., -1.2986, -1.3413, -1.2439],\n",
       "          [-0.7350, -0.7098, -0.7406,  ..., -1.2645, -1.2234, -1.2507]]]),\n",
       " tensor([[[ 0.3742,  0.3873,  0.4288,  ..., -0.4091, -0.5045, -0.6994],\n",
       "          [ 0.3800,  0.3752,  0.4161,  ..., -0.4810, -0.5131, -0.6560],\n",
       "          [ 0.4215,  0.4170,  0.4508,  ..., -0.5058, -0.5065, -0.5556],\n",
       "          ...,\n",
       "          [-1.9124, -1.9128, -1.8646,  ..., -1.3326, -1.3354, -1.4256],\n",
       "          [-1.9380, -1.9085, -1.9010,  ..., -1.3102, -1.3713, -1.4205],\n",
       "          [-1.9503, -1.9453, -1.9295,  ..., -1.3681, -1.4036, -1.4537]],\n",
       " \n",
       "         [[ 0.6108,  0.6242,  0.6666,  ..., -0.2538, -0.3513, -0.3893],\n",
       "          [ 0.6805,  0.6756,  0.7174,  ..., -0.3064, -0.3375, -0.3663],\n",
       "          [ 0.7354,  0.7308,  0.7654,  ..., -0.3176, -0.3183, -0.2848],\n",
       "          ...,\n",
       "          [-1.8081, -1.8086, -1.7593,  ..., -1.3018, -1.3058, -1.3474],\n",
       "          [-1.8343, -1.8041, -1.7964,  ..., -1.2962, -1.3304, -1.3316],\n",
       "          [-1.8331, -1.8280, -1.8119,  ..., -1.3577, -1.3417, -1.3392]],\n",
       " \n",
       "         [[ 0.8963,  0.9096,  0.9519,  ..., -0.0478, -0.1449, -0.2350],\n",
       "          [ 0.9407,  0.9359,  0.9776,  ..., -0.1002, -0.1312, -0.2034],\n",
       "          [ 0.9892,  0.9847,  1.0191,  ..., -0.1114, -0.1121, -0.1123],\n",
       "          ...,\n",
       "          [-1.5081, -1.5086, -1.4595,  ..., -1.1284, -1.1300, -1.2063],\n",
       "          [-1.5467, -1.5166, -1.5090,  ..., -1.1205, -1.1237, -1.2165],\n",
       "          [-1.6090, -1.6039, -1.5878,  ..., -1.1788, -1.1421, -1.2504]]]),\n",
       " tensor([[[ 1.2145,  1.2060,  0.9256,  ...,  0.8150,  1.1796,  0.7438],\n",
       "          [ 1.3205,  1.2273,  1.3322,  ...,  1.0983,  1.0682,  0.1614],\n",
       "          [ 1.3425,  1.2824,  1.3393,  ...,  1.1982,  0.3773,  0.7132],\n",
       "          ...,\n",
       "          [ 1.0923,  1.1334,  0.8449,  ...,  0.8376,  0.4599,  0.6277],\n",
       "          [ 0.7175,  0.8535,  1.0183,  ...,  0.5555,  1.2331,  0.7534],\n",
       "          [ 0.8034,  0.9363,  0.4719,  ...,  0.6664,  0.1496,  0.1624]],\n",
       " \n",
       "         [[ 1.7700,  1.6650,  1.1905,  ...,  1.0257,  1.4201,  0.8686],\n",
       "          [ 1.8222,  1.6590,  1.6572,  ...,  1.2537,  1.2461,  0.2433],\n",
       "          [ 1.8218,  1.6856,  1.7251,  ...,  1.3346,  0.4815,  0.8061],\n",
       "          ...,\n",
       "          [ 0.9310,  0.9906,  0.7131,  ...,  0.7220,  0.2732,  0.4560],\n",
       "          [ 0.5478,  0.7147,  0.9238,  ...,  0.4032,  1.0225,  0.6034],\n",
       "          [ 0.6532,  0.8178,  0.3540,  ...,  0.4421, -0.1091,  0.0229]],\n",
       " \n",
       "         [[ 2.3152,  2.1128,  1.6218,  ...,  1.4395,  1.8229,  1.1666],\n",
       "          [ 2.3249,  2.0708,  2.1153,  ...,  1.6233,  1.6455,  0.5837],\n",
       "          [ 2.2911,  2.0687,  2.2185,  ...,  1.6566,  0.8895,  1.1816],\n",
       "          ...,\n",
       "          [ 0.7637,  0.7552,  0.4790,  ...,  0.5725,  0.1793,  0.3201],\n",
       "          [ 0.3493,  0.4806,  0.6779,  ...,  0.2272,  0.9314,  0.5392],\n",
       "          [ 0.4194,  0.5497,  0.1596,  ...,  0.3218, -0.1455,  0.0184]]]),\n",
       " tensor([[[ 0.2456,  0.2456,  0.2781,  ...,  0.0567,  0.0569,  0.0397],\n",
       "          [ 0.2796,  0.2624,  0.2305,  ...,  0.0239,  0.0569,  0.0398],\n",
       "          [ 0.2658,  0.2453,  0.2109,  ...,  0.0413,  0.0242,  0.0056],\n",
       "          ...,\n",
       "          [-0.0369, -0.1362, -0.1827,  ..., -0.8372, -1.1663, -0.5717],\n",
       "          [-0.1668, -0.1813, -0.2654,  ..., -0.9349, -1.0982, -1.5491],\n",
       "          [-0.3056, -0.1238, -0.1669,  ..., -0.8366, -1.0489, -1.1925]],\n",
       " \n",
       "         [[ 0.5556,  0.5556,  0.5728,  ...,  0.4325,  0.4328,  0.4152],\n",
       "          [ 0.5728,  0.5728,  0.5892,  ...,  0.3990,  0.4328,  0.4153],\n",
       "          [ 0.5519,  0.5553,  0.6051,  ...,  0.4168,  0.3993,  0.3803],\n",
       "          ...,\n",
       "          [-0.1184, -0.1642, -0.1799,  ..., -0.8868, -1.2158, -0.5851],\n",
       "          [-0.1986, -0.2134, -0.2644,  ..., -0.9838, -1.1509, -1.5867],\n",
       "          [-0.2883, -0.1199, -0.2466,  ..., -0.8834, -1.1114, -1.2222]],\n",
       " \n",
       "         [[ 0.9323,  0.9323,  0.9494,  ...,  0.8445,  0.8448,  0.8273],\n",
       "          [ 0.9494,  0.9494,  0.9494,  ...,  0.8112,  0.8448,  0.8274],\n",
       "          [ 0.9319,  0.9319,  0.9479,  ...,  0.8289,  0.8115,  0.7925],\n",
       "          ...,\n",
       "          [-0.1396, -0.2042, -0.2675,  ..., -0.9586, -1.2511, -0.5887],\n",
       "          [-0.2369, -0.2517, -0.3547,  ..., -1.0535, -1.1850, -1.5878],\n",
       "          [-0.3436, -0.1759, -0.3037,  ..., -0.9535, -1.1127, -1.2175]]])]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda x: transform(Image.open(x)), a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "496e6ccc940d7baf27126816b5de78ce76f9b9d17be85a2d2d9dfc63004cf58f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('cama')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
