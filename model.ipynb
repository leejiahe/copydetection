{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import einops\n",
    "from PIL import Image\n",
    "from typing import Any, List, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from torchmetrics import MaxMetric\n",
    "from torchmetrics.classification.accuracy import Accuracy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from transformers import ViTModel\n",
    "\n",
    "from src.models.components.layers import CopyDetectEmbedding, NormalizedFeatures, SimImagePred, ContrastiveProj\n",
    "from src.datamodules.copydetect_datamodule import CopyDetectDataModule\n",
    "from src.datamodules.components.augmentation import Augment\n",
    "from src.utils.nt_xent_loss import NTXentLoss\n",
    "\n",
    "\n",
    "get_path = lambda x: os.path.join(os.getcwd(),'data', x)\n",
    "\n",
    "augment = Augment(overlay_image_dir = get_path('train/'),\n",
    "                  n_upper = 2,\n",
    "                  n_lower = 1)\n",
    "\n",
    "ntxentloss = NTXentLoss(temperature = 0.9, eps = 1e-5)\n",
    "\n",
    "device = torch.device('cuda:4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/vit-base-patch16-224 were not used when initializing ViTModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "class CopyDetectModule(LightningModule):\n",
    "    def __init__(self,\n",
    "                 pretrained_arch: str,          # Pretrained ViT architecture\n",
    "                 ntxentloss: object,            # Contrastive loss\n",
    "                 hidden_dim: int = 2048,        # Contrastive projection size of hidden layer\n",
    "                 projected_dim: int = 512,      # Contrastive projection size of projection head \n",
    "                 beta1: int = 1,                # Similar image BCE loss multiplier\n",
    "                 beta2: int = 1,                # Contrastive loss multiplier\n",
    "                 lr: float = 0.001,\n",
    "                 weight_decay: float = 0.0005):               \n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(logger = False)\n",
    "         \n",
    "        # Instantiate ViT encoder from pretrained model\n",
    "        pretrained_model = ViTModel.from_pretrained(pretrained_arch)\n",
    "        encoder = pretrained_model.encoder\n",
    "        self.patch_size = pretrained_model.config.patch_size\n",
    "                \n",
    "        # Instantiate embedding, we use the pretrained ViT cls and position embedding\n",
    "        embedding = CopyDetectEmbedding(config = pretrained_model.config,\n",
    "                                        vit_cls = pretrained_model.embeddings.cls_token,\n",
    "                                        pos_emb = pretrained_model.embeddings.position_embeddings)\n",
    "        \n",
    "        # Normalized features\n",
    "        normfeats = NormalizedFeatures(hidden_dim = pretrained_model.config.hidden_size,\n",
    "                                       layer_norm_eps = pretrained_model.config.layer_norm_eps)\n",
    "        # Feature Vector Extractor\n",
    "        self.feature_extractor = nn.Sequential(embedding, encoder, normfeats)\n",
    "        \n",
    "        # Instantiate SimImagePredictor\n",
    "        simimagepred = SimImagePred(embedding_dim = pretrained_model.config.hidden_size)\n",
    "        self.embedding = embedding\n",
    "        self.simimagepred = nn.Sequential(encoder, normfeats, simimagepred)\n",
    "\n",
    "        # Instantiate ContrastiveProjection\n",
    "        contrastiveproj = ContrastiveProj(embedding_dim = pretrained_model.config.hidden_size,\n",
    "                                          hidden_dim = hidden_dim,\n",
    "                                          projected_dim = projected_dim)\n",
    "        self.contrastiveproj = nn.Sequential(embedding, encoder, normfeats, contrastiveproj)\n",
    "        \n",
    "        # Contrastive loss \n",
    "        self.contrastive_loss = ntxentloss\n",
    "        \n",
    "        # Binary cross entropy loss for similar image pair\n",
    "        self.bce_loss = torch.nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        # Model accuracy in detecting modified copy\n",
    "        self.train_acc, self.val_acc = Accuracy(), Accuracy()   \n",
    "          \n",
    "        # For logging best validation accuracy\n",
    "        self.val_acc_best = MaxMetric()\n",
    "\n",
    "    def feature_extract(self, batch: Any) -> torch.Tensor:\n",
    "        # To extract feature vector\n",
    "        img_r, img_id = batch\n",
    "        encoding = self.feature_extractor(img_r)\n",
    "        batch_size, num_ch, H, W, = img_r.size()\n",
    "        #dim = encoding.size(2) # batch_size, seq_len, dim \n",
    "        h, w = int(H/self.patch_size), int(W/self.patch_size)\n",
    "        cls, feats = encoding[:,0,:], encoding[:,1:,:] # Get the cls token and all the images features\n",
    "            \n",
    "        #feats = feats.reshape(batch_size, h, w, dim).clamp(min = 1e-6).permute(0,3,1,2)\n",
    "        feats = einops.rearrange(feats, 'b (h w) d -> b d h w', h = h, w = w).clamp(min = 1e-6)\n",
    "        # GeM Pooling\n",
    "        feats = F.avg_pool2d(feats.pow(4), (h,w)).pow(1./4)\n",
    "        feats = einops.rearrange(feats, 'b d () () -> b d')\n",
    "        # Concatenate cls tokens with image patches to give local and global views of image\n",
    "        feature_vector = torch.cat((cls, feats), dim = 1)\n",
    "\n",
    "        return feature_vector, img_id\n",
    "        \n",
    "    def predict_copy(self, batch):\n",
    "        # For copy detection \n",
    "        img_r, img_q = batch\n",
    "        embedding_rq = self.embedding(img_r, img_q)\n",
    "        logits = self.simimagepred(embedding_rq)\n",
    "        preds = torch.argmax(logits, dim = 1)\n",
    "        return preds\n",
    "\n",
    "    def step(self,\n",
    "             img_r: List[torch.Tensor],\n",
    "             img_q: List[torch.Tensor],\n",
    "             label: List[torch.Tensor]):\n",
    "        \n",
    "        # img_r, img_q to SimImagePredictor\n",
    "        embedding_rq = self.embedding(img_r, img_q) ## nn sequential don't take multiple input\n",
    "        logits = self.simimagepred(embedding_rq)\n",
    "        # Calculate binary cross entropy loss of similar image pair\n",
    "        simimage_loss = self.bce_loss(logits, label.unsqueeze(dim = 1))\n",
    "        # Predictions\n",
    "        preds = torch.argmax(logits, dim = 1)\n",
    "        \n",
    "        # Get positive indices\n",
    "        pos_indices = label.bool()\n",
    "        # Forward positive indices of img_r and img_q to ContrastiveProjection\n",
    "        proj_r = self.contrastiveproj(img_r[pos_indices])\n",
    "        proj_q = self.contrastiveproj(img_q[pos_indices])\n",
    "\n",
    "        # Calculate contrastive loss between un-augmented img_r and augmented positive pair of img_q\n",
    "        contrastive_loss = self.contrastive_loss(proj_r, proj_q)\n",
    "        \n",
    "        # Weighted sum of bce and contrastive loss\n",
    "        total_loss = self.hparams.beta1 * simimage_loss + self.hparams.beta2 * contrastive_loss\n",
    "        \n",
    "        return {'simimage': simimage_loss, 'contrastive': contrastive_loss, 'total': total_loss}, preds\n",
    "\n",
    "    def training_step(self, batch: Any, batch_idx: int):\n",
    "        img_r, img_q, label = batch\n",
    "        img_r, img_q, label = torch.vstack(img_r), torch.vstack(img_q), torch.hstack(label)\n",
    "\n",
    "        losses, preds = self.step(img_r, img_q, label)\n",
    "        \n",
    "        # Log train metrics\n",
    "        acc = self.train_acc(preds, label.int())\n",
    "        self.log(\"train/total_loss\", losses['total'], on_step = True, on_epoch = True, prog_bar = False)\n",
    "        self.log(\"train/simimage_loss\", losses['simimage'], on_step = True, on_epoch = True, prog_bar = False)\n",
    "        self.log(\"train/contrastive_loss\", losses['contrastive'], on_step = True, on_epoch = True, prog_bar = False)\n",
    "        self.log(\"train/acc\", acc, on_step = True, on_epoch = True, prog_bar = True)\n",
    "\n",
    "        return losses['total']\n",
    "\n",
    "    def validation_step(self, batch: Any, batch_idx: int):\n",
    "        img_r, img_q, label = batch\n",
    "        losses, preds = self.step(img_r, img_q, label)\n",
    "\n",
    "        # Log val metrics\n",
    "        acc = self.val_acc(preds, label.int())\n",
    "        self.log(\"val/total_loss\", losses['total'], on_step = True, on_epoch = True, prog_bar = False)\n",
    "        self.log(\"val/simimage_loss\", losses['simimage'], on_step = True, on_epoch = True, prog_bar = False)\n",
    "        self.log(\"val/contrastive_loss\", losses['contrastive'], on_step = True, on_epoch = True, prog_bar = False)\n",
    "        self.log(\"val/acc\", acc, on_step = True, on_epoch = True, prog_bar = True)\n",
    "\n",
    "        return losses['total']\n",
    "\n",
    "    def validation_epoch_end(self, outputs: Any):\n",
    "        acc = self.val_acc.compute()  # get val accuracy from current epoch\n",
    "        self.val_acc_best.update(acc)\n",
    "        self.log(\"val/acc_best\", self.val_acc_best.compute(), on_epoch = True, prog_bar = True)\n",
    "        \n",
    "    def test_step(self, batch: Any, batch_idx: int):\n",
    "        feats = self.feature_extract(batch) # Get feat\n",
    "        \n",
    "        return feats\n",
    "    \n",
    "    def test_epoch_end(self, test_step_outputs: Any):\n",
    "        all_feats, all_ids = [], []\n",
    "        for step_output in test_step_outputs:\n",
    "            all_feats.append(step_output[0])\n",
    "            all_ids.extend(step_output[1])\n",
    "            \n",
    "        all_feats = torch.vstack(all_feats)\n",
    "        self.test_results = (all_feats, all_ids)\n",
    "        \n",
    "        return all_feats\n",
    "    \n",
    "    def predict_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0):\n",
    "        self.test_results = None\n",
    "        score = self.predict_copy(batch)\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        # Reset metrics at the end of every epoch\n",
    "        self.train_acc.reset()\n",
    "        self.val_acc.reset()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(params = self.parameters(),\n",
    "                                lr = self.hparams.lr,\n",
    "                                weight_decay = self.hparams.weight_decay)\n",
    "        \n",
    "pretrained_arch = 'google/vit-base-patch16-224'\n",
    "\n",
    "model = CopyDetectModule(pretrained_arch, ntxentloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = CopyDetectDataModule(train_dir = get_path('train/'),\n",
    "                                  references_dir = get_path('references/'),\n",
    "                                  dev_queries_dir = get_path('dev_queries/'),\n",
    "                                  final_queries_dir = get_path('final_queries/'),\n",
    "                                  augment = augment,\n",
    "                                  dev_validation_set = get_path('dev_validation_set.csv'),\n",
    "                                  batch_size = 16,\n",
    "                                  pin_memory = True,\n",
    "                                  num_workers = 10,\n",
    "                                  n_crops = 2)\n",
    "\n",
    "datamodule.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(accelerator = 'gpu', devices = [4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86ffb988e2e14c5ebc49463aadddf633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trainer.test(model = model, dataloaders = datamodule.references_dataloader())\n",
    "r, r_id = model.test_results\n",
    "r = r.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bd21c914f3c48a7838036b83fea1a2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trainer.test(model = model, dataloaders = datamodule.final_queries_dataloader())\n",
    "q, q_id = model.test_results\n",
    "q = q.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_SDEV = [0.229, 0.224, 0.225]\n",
    "\n",
    "transform =  transforms.Compose([transforms.ToTensor(),\n",
    "                                 transforms.Resize((224, 224)),\n",
    "                                 transforms.Normalize(IMAGENET_MEAN, IMAGENET_SDEV)])\n",
    "\n",
    "get_image = lambda img_dir, img: Image.open(os.path.join(img_dir, img))\n",
    "get_image_file = lambda image_dir: [os.path.join(image_dir, f) for f in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, f))]\n",
    "\n",
    "class CopyDetectPredDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 predictions: list,\n",
    "                 references_dir: str,\n",
    "                 final_queries_dir: str):\n",
    "        self.predictions = predictions\n",
    "        self.references_dir = references_dir\n",
    "        self.final_queries_dir = final_queries_dir\n",
    "        self.transform =  transforms.Compose([transforms.ToTensor(),\n",
    "                                              transforms.Resize((224, 224)),\n",
    "                                              transforms.Normalize(IMAGENET_MEAN, IMAGENET_SDEV)])\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.predictions)\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        final_queries_id, references_id = self.predictions[index][0], self.predictions[index][1]\n",
    "        \n",
    "        reference_image = get_image(self.references_dir, references_id)\n",
    "        final_queries_image = get_image(self.final_queries_dir, final_queries_id)\n",
    "        \n",
    "        return self.transform(reference_image), self.transform(final_queries_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import search_with_capped_res\n",
    "\n",
    "lims, dis, ids = search_with_capped_res(q, r, 1000)\n",
    "\n",
    "predictions_list = []\n",
    "for i in range(100):\n",
    "    for j in range(lims[i], lims[i+1]):\n",
    "        predictions_list.append([q_id[i], r_id[ids[j]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "copydetectpred = CopyDetectPredDataset(predictions = predictions_list,\n",
    "                                       references_dir = get_path('references'),\n",
    "                                       final_queries_dir = get_path('final_queries/'))\n",
    "\n",
    "\n",
    "copydetect_dataloader = DataLoader(dataset = copydetectpred,\n",
    "                                   batch_size = 16,\n",
    "                                   num_workers = 8,\n",
    "                                   pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ba89607162e46ba9031ed4a69bdb6fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    }
   ],
   "source": [
    "p = trainer.predict(model = model, dataloaders = copydetect_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.hstack(p).cpu().numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/leejiahe/copydetection/model.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bidsd-1.d2.comp.nus.edu.sg/home/leejiahe/copydetection/model.ipynb#ch0000031vscode-remote?line=0'>1</a>\u001b[0m ref_feats, ref_ids \u001b[39m=\u001b[39m [], []\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bidsd-1.d2.comp.nus.edu.sg/home/leejiahe/copydetection/model.ipynb#ch0000031vscode-remote?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m datamodule\u001b[39m.\u001b[39mreferences_dataloader():\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bidsd-1.d2.comp.nus.edu.sg/home/leejiahe/copydetection/model.ipynb#ch0000031vscode-remote?line=3'>4</a>\u001b[0m     feats, img_id \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfeature_extract(batch)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bidsd-1.d2.comp.nus.edu.sg/home/leejiahe/copydetection/model.ipynb#ch0000031vscode-remote?line=4'>5</a>\u001b[0m     ref_feats\u001b[39m.\u001b[39mappend(feats)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bidsd-1.d2.comp.nus.edu.sg/home/leejiahe/copydetection/model.ipynb#ch0000031vscode-remote?line=5'>6</a>\u001b[0m     ref_ids\u001b[39m.\u001b[39mextend(img_id)\n",
      "\u001b[1;32m/home/leejiahe/copydetection/model.ipynb Cell 2'\u001b[0m in \u001b[0;36mCopyDetectModule.feature_extract\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bidsd-1.d2.comp.nus.edu.sg/home/leejiahe/copydetection/model.ipynb#ch0000017vscode-remote?line=52'>53</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfeature_extract\u001b[39m(\u001b[39mself\u001b[39m, batch: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bidsd-1.d2.comp.nus.edu.sg/home/leejiahe/copydetection/model.ipynb#ch0000017vscode-remote?line=53'>54</a>\u001b[0m     \u001b[39m# To extract feature vector\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bidsd-1.d2.comp.nus.edu.sg/home/leejiahe/copydetection/model.ipynb#ch0000017vscode-remote?line=54'>55</a>\u001b[0m     img_r, img_id \u001b[39m=\u001b[39m batch\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bidsd-1.d2.comp.nus.edu.sg/home/leejiahe/copydetection/model.ipynb#ch0000017vscode-remote?line=55'>56</a>\u001b[0m     encoding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeature_extractor(img_r)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bidsd-1.d2.comp.nus.edu.sg/home/leejiahe/copydetection/model.ipynb#ch0000017vscode-remote?line=56'>57</a>\u001b[0m     batch_size, num_ch, H, W, \u001b[39m=\u001b[39m img_r\u001b[39m.\u001b[39msize()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bidsd-1.d2.comp.nus.edu.sg/home/leejiahe/copydetection/model.ipynb#ch0000017vscode-remote?line=57'>58</a>\u001b[0m     \u001b[39m#dim = encoding.size(2) # batch_size, seq_len, dim \u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/container.py?line=136'>137</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/container.py?line=137'>138</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/container.py?line=138'>139</a>\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/container.py?line=139'>140</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/cama/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py:377\u001b[0m, in \u001b[0;36mViTEncoder.forward\u001b[0;34m(self, hidden_states, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py?line=370'>371</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py?line=371'>372</a>\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py?line=372'>373</a>\u001b[0m         hidden_states,\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py?line=373'>374</a>\u001b[0m         layer_head_mask,\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py?line=374'>375</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py?line=375'>376</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py?line=376'>377</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(hidden_states, layer_head_mask, output_attentions)\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py?line=378'>379</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py?line=380'>381</a>\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/cama/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py:332\u001b[0m, in \u001b[0;36mViTLayer.forward\u001b[0;34m(self, hidden_states, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py?line=328'>329</a>\u001b[0m layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate(layer_output)\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py?line=330'>331</a>\u001b[0m \u001b[39m# second residual connection is done here\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py?line=331'>332</a>\u001b[0m layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput(layer_output, hidden_states)\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py?line=333'>334</a>\u001b[0m outputs \u001b[39m=\u001b[39m (layer_output,) \u001b[39m+\u001b[39m outputs\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py?line=335'>336</a>\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/cama/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py:294\u001b[0m, in \u001b[0;36mViTOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py?line=292'>293</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states, input_tensor):\n\u001b[0;32m--> <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py?line=293'>294</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdense(hidden_states)\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py?line=294'>295</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py?line=296'>297</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m hidden_states \u001b[39m+\u001b[39m input_tensor\n",
      "File \u001b[0;32m~/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/linear.py:96\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/linear.py?line=94'>95</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/linear.py?line=95'>96</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/functional.py:1847\u001b[0m, in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/functional.py?line=1844'>1845</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight):\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/functional.py?line=1845'>1846</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[39minput\u001b[39m, weight), \u001b[39minput\u001b[39m, weight, bias\u001b[39m=\u001b[39mbias)\n\u001b[0;32m-> <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/functional.py?line=1846'>1847</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, weight, bias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ref_feats, ref_ids = [], []\n",
    "\n",
    "for batch in datamodule.references_dataloader():\n",
    "    feats, img_id = model.feature_extract(batch)\n",
    "    ref_feats.append(feats)\n",
    "    ref_ids.extend(img_id)\n",
    "ref_feats = torch.vstack(ref_feats).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/leejiahe/copydetection/model.ipynb Cell 15'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bidsd-1.d2.comp.nus.edu.sg/home/leejiahe/copydetection/model.ipynb#ch0000080vscode-remote?line=0'>1</a>\u001b[0m model(img)\n",
      "File \u001b[0;32m~/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/cama/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py:629\u001b[0m, in \u001b[0;36mLightningModule.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py?line=617'>618</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py?line=618'>619</a>\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py?line=619'>620</a>\u001b[0m \u001b[39m    Same as :meth:`torch.nn.Module.forward()`.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py?line=620'>621</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py?line=626'>627</a>\u001b[0m \u001b[39m        Your model's output\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py?line=627'>628</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py?line=628'>629</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py:201\u001b[0m, in \u001b[0;36m_forward_unimplemented\u001b[0;34m(self, *input)\u001b[0m\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=189'>190</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_forward_unimplemented\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39minput\u001b[39m: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=190'>191</a>\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Defines the computation performed at every call.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=191'>192</a>\u001b[0m \n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=192'>193</a>\u001b[0m \u001b[39m    Should be overridden by all subclasses.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=198'>199</a>\u001b[0m \u001b[39m        registered hooks while the latter silently ignores them.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=199'>200</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/nn/modules/module.py?line=200'>201</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_feats, query_ids = [], []\n",
    "\n",
    "for batch in datamodule.final_queries_dataloader():\n",
    "    img, img_id = batch\n",
    "    feats = model(img.to(device))\n",
    "    query_feats.append(feats)\n",
    "    query_ids.extend(img_id)\n",
    "    \n",
    "query_feats = torch.vstack(query_feats).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import search_with_capped_res\n",
    "\n",
    "lims, dis, ids = search_with_capped_res(query_feats, ref_feats, 1000)\n",
    "\n",
    "predictions_list = []\n",
    "for i in range(100):\n",
    "    for j in range(lims[i], lims[i+1]):\n",
    "        predictions_list.append([query_ids[i], ref_ids[ids[j]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.test_results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for searches in copydetect_dataloader:\n",
    "    query_id, ref_id = searches\n",
    "    query_id, ref_id = query_id.to(device), ref_id.to(device)\n",
    "    score = model(ref_id, query_id)\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = torch.hstack(scores).detach().cpu().numpy()\n",
    "scores.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "496e6ccc940d7baf27126816b5de78ce76f9b9d17be85a2d2d9dfc63004cf58f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('cama')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
