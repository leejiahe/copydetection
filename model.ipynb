{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import einops\n",
    "from PIL import Image\n",
    "from typing import Any, List, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from torchmetrics import MaxMetric\n",
    "from torchmetrics.classification.accuracy import Accuracy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "from transformers import ViTModel\n",
    "\n",
    "from src.models.components.layers import CopyDetectEmbedding, NormalizedFeatures, SimImagePred, ContrastiveProj\n",
    "from src.datamodules.copydetect_datamodule import CopyDetectDataModule\n",
    "from src.datamodules.components.augmentation import Augment\n",
    "from src.utils.nt_xent_loss import NTXentLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "get_path = lambda x: os.path.join(os.getcwd(),'data', x)\n",
    "\n",
    "augment = Augment(overlay_image_dir = get_path('train/'),\n",
    "                  n_upper = 2,\n",
    "                  n_lower = 1)\n",
    "ntxentloss = NTXentLoss(temperature = 0.9, eps = 1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CopyDetectModule(LightningModule):\n",
    "    def __init__(self,\n",
    "                 pretrained_arch: str,          # Pretrained ViT architecture\n",
    "                 ntxentloss: object,            # Contrastive loss\n",
    "                 hidden_dim: int = 2048,        # Contrastive projection size of hidden layer\n",
    "                 projected_dim: int = 512,      # Contrastive projection size of projection head \n",
    "                 beta1: int = 1,                # Similar image BCE loss multiplier\n",
    "                 beta2: int = 1,                # Contrastive loss multiplier\n",
    "                 lr: float = 0.001,\n",
    "                 weight_decay: float = 0.0005):               \n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(logger = False)\n",
    "         \n",
    "        # Instantiate ViT encoder from pretrained model\n",
    "        pretrained_model = ViTModel.from_pretrained(pretrained_arch)\n",
    "        encoder = pretrained_model.encoder\n",
    "        self.patch_size = pretrained_model.config.patch_size\n",
    "                \n",
    "        # Instantiate embedding, we use the pretrained ViT cls and position embedding\n",
    "        embedding = CopyDetectEmbedding(config = pretrained_model.config,\n",
    "                                        vit_cls = pretrained_model.embeddings.cls_token,\n",
    "                                        pos_emb = pretrained_model.embeddings.position_embeddings)\n",
    "        \n",
    "        # Normalized features\n",
    "        normfeats = NormalizedFeatures(hidden_dim = pretrained_model.config.hidden_size,\n",
    "                                       layer_norm_eps = pretrained_model.config.layer_norm_eps)\n",
    "        # Feature Vector Extractor\n",
    "        self.feature_extractor = nn.Sequential(embedding, encoder, normfeats)\n",
    "        \n",
    "        # Instantiate SimImagePredictor\n",
    "        simimagepred = SimImagePred(embedding_dim = pretrained_model.config.hidden_size)\n",
    "        self.embedding = embedding\n",
    "        self.simimagepred = nn.Sequential(encoder, normfeats, simimagepred)\n",
    "\n",
    "        # Instantiate ContrastiveProjection\n",
    "        contrastiveproj = ContrastiveProj(embedding_dim = pretrained_model.config.hidden_size,\n",
    "                                          hidden_dim = hidden_dim,\n",
    "                                          projected_dim = projected_dim)\n",
    "        self.contrastiveproj = nn.Sequential(embedding, encoder, normfeats, contrastiveproj)\n",
    "        \n",
    "        # Contrastive loss \n",
    "        self.contrastive_loss = ntxentloss\n",
    "        \n",
    "        # Binary cross entropy loss for similar image pair\n",
    "        self.bce_loss = torch.nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        # Model accuracy in detecting modified copy\n",
    "        self.train_acc, self.val_acc = Accuracy(), Accuracy()   \n",
    "          \n",
    "        # For logging best validation accuracy\n",
    "        self.val_acc_best = MaxMetric()\n",
    "\n",
    "    def forward(self,\n",
    "                img_r: torch.Tensor,\n",
    "                img_q: Optional[torch.Tensor] = None,\n",
    "                ) -> torch.Tensor:\n",
    "        \n",
    "        if img_q is None:\n",
    "            encoding = self.feature_extractor(img_r)\n",
    "            batch_size, num_ch, H, W, = img_r.size()\n",
    "            #dim = encoding.size(2) # batch_size, seq_len, dim \n",
    "            h, w = int(H/self.patch_size), int(W/self.patch_size)\n",
    "            cls, feats = encoding[:,0,:], encoding[:,1:,:] # Get the cls token and all the images features\n",
    "            \n",
    "            #feats = feats.reshape(batch_size, h, w, dim).clamp(min = 1e-6).permute(0,3,1,2)\n",
    "            feats = einops.rearrange(feats, 'b (h w) d -> b d h w', h = h, w = w).clamp(min = 1e-6)\n",
    "            # GeM Pooling\n",
    "            feats = F.avg_pool2d(feats.pow(4), (h,w)).pow(1./4)\n",
    "            feats = einops.rearrange(feats, 'b d () () -> b d')\n",
    "            # Concatenate cls tokens with image patches to give local and global views of image\n",
    "            feature_vector = torch.cat((cls, feats), dim = 1)\n",
    "            return feature_vector\n",
    "        else:\n",
    "            embedding_rq = self.embedding(img_r, img_q)\n",
    "            logits = self.simimagepred(embedding_rq)\n",
    "            preds = torch.argmax(logits, dim = 1)\n",
    "            return preds\n",
    "\n",
    "    def step(self,\n",
    "             img_r: List[torch.Tensor],\n",
    "             img_q: List[torch.Tensor],\n",
    "             label: List[torch.Tensor]):\n",
    "        \n",
    "        # img_r, img_q to SimImagePredictor\n",
    "        embedding_rq = self.embedding(img_r, img_q) ## nn sequential don't take multiple input\n",
    "        logits = self.simimagepred(embedding_rq)\n",
    "        # Calculate binary cross entropy loss of similar image pair\n",
    "        simimage_loss = self.bce_loss(logits, label.unsqueeze(dim = 1))\n",
    "        # Predictions\n",
    "        preds = torch.argmax(logits, dim = 1)\n",
    "        \n",
    "        # Get positive indices\n",
    "        pos_indices = label.bool()\n",
    "        # Forward positive indices of img_r and img_q to ContrastiveProjection\n",
    "        proj_r = self.contrastiveproj(img_r[pos_indices])\n",
    "        proj_q = self.contrastiveproj(img_q[pos_indices])\n",
    "\n",
    "        # Calculate contrastive loss between un-augmented img_r and augmented positive pair of img_q\n",
    "        contrastive_loss = self.contrastive_loss(proj_r, proj_q)\n",
    "        \n",
    "        # Weighted sum of bce and contrastive loss\n",
    "        total_loss = self.hparams.beta1 * simimage_loss + self.hparams.beta2 * contrastive_loss\n",
    "        \n",
    "        return {'simimage': simimage_loss, 'contrastive': contrastive_loss, 'total': total_loss}, preds\n",
    "\n",
    "    def training_step(self, batch: Any, batch_idx: int):\n",
    "        img_r, img_q, label = batch\n",
    "        img_r, img_q, label = torch.vstack(img_r), torch.vstack(img_q), torch.hstack(label)\n",
    "\n",
    "        losses, preds = self.step(img_r, img_q, label)\n",
    "        \n",
    "        # Log train metrics\n",
    "        acc = self.train_acc(preds, label.int())\n",
    "        self.log(\"train/total_loss\", losses['total'], on_step = True, on_epoch = True, prog_bar = False)\n",
    "        self.log(\"train/simimage_loss\", losses['simimage'], on_step = True, on_epoch = True, prog_bar = False)\n",
    "        self.log(\"train/contrastive_loss\", losses['contrastive'], on_step = True, on_epoch = True, prog_bar = False)\n",
    "        self.log(\"train/acc\", acc, on_step = True, on_epoch = True, prog_bar = True)\n",
    "\n",
    "        return losses['total']\n",
    "\n",
    "    def validation_step(self, batch: Any, batch_idx: int):\n",
    "        img_r, img_q, label = batch\n",
    "        losses, preds = self.step(img_r, img_q, label)\n",
    "\n",
    "        # Log val metrics\n",
    "        acc = self.val_acc(preds, label.int())\n",
    "        self.log(\"val/total_loss\", losses['total'], on_step = True, on_epoch = True, prog_bar = False)\n",
    "        self.log(\"val/simimage_loss\", losses['simimage'], on_step = True, on_epoch = True, prog_bar = False)\n",
    "        self.log(\"val/contrastive_loss\", losses['contrastive'], on_step = True, on_epoch = True, prog_bar = False)\n",
    "        self.log(\"val/acc\", acc, on_step = True, on_epoch = True, prog_bar = True)\n",
    "\n",
    "        return losses['total']\n",
    "\n",
    "    def validation_epoch_end(self, outputs: Any):\n",
    "        acc = self.val_acc.compute()  # get val accuracy from current epoch\n",
    "        self.val_acc_best.update(acc)\n",
    "        self.log(\"val/acc_best\", self.val_acc_best.compute(), on_epoch = True, prog_bar = True)\n",
    "        \n",
    "    def test_step(self, batch: Any, batch_idx: int):\n",
    "        feats = self.forward(batch) # Get feat\n",
    "        return feats\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # Reset metrics at the end of every epoch\n",
    "        self.train_acc.reset()\n",
    "        self.val_acc.reset()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(params = self.parameters(),\n",
    "                                lr = self.hparams.lr,\n",
    "                                weight_decay = self.hparams.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/vit-base-patch16-224 were not used when initializing ViTModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.weight', 'vit.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pretrained_arch = 'google/vit-base-patch16-224'\n",
    "\n",
    "model = CopyDetectModule(pretrained_arch, ntxentloss)\n",
    "\n",
    "datamodule = CopyDetectDataModule(train_dir = get_path('train/'),\n",
    "                           references_dir = get_path('references/'),\n",
    "                           dev_queries_dir = get_path('dev_queries/'),\n",
    "                           final_queries_dir = get_path('final_queries/'),\n",
    "                           augment = augment,\n",
    "                           dev_validation_set = get_path('dev_validation_set.csv'),\n",
    "                           batch_size = 16,\n",
    "                           pin_memory = True,\n",
    "                           num_workers = 10,\n",
    "                           n_crops = 2)\n",
    "\n",
    "datamodule.setup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_SDEV = [0.229, 0.224, 0.225]\n",
    "\n",
    "transform =  transforms.Compose([transforms.ToTensor(),\n",
    "                                 transforms.Resize((224, 224)),\n",
    "                                 transforms.Normalize(IMAGENET_MEAN, IMAGENET_SDEV)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(datamodule.references_dataloader())).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#trainer = Trainer(fast_dev_run =  True)\n",
    "#trainer.fit(model = model, datamodule = datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_image = lambda folder, img: Image.open(folder[index])\n",
    "get_image_file = lambda image_dir: [os.path.join(image_dir, f) for f in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, f))]\n",
    "\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_SDEV = [0.229, 0.224, 0.225]\n",
    "\n",
    "class CopyDetectPredDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 predictions: list,\n",
    "                 references_dir: str,\n",
    "                 final_queries_dir: str):\n",
    "        self.predictions = predictions\n",
    "        self.references_dir = references_dir\n",
    "        self.final_queries_dir = final_queries_dir\n",
    "        self.transform =  transforms.Compose([transforms.ToTensor(),\n",
    "                                              transforms.Resize((224, 224)),\n",
    "                                              transforms.Normalize(IMAGENET_MEAN, IMAGENET_SDEV)])\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.predictions)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> torch.Tensor:\n",
    "        final_queries_id, references_id = self.predictions[index]\n",
    "        reference_image = get_image(self.references_images, index)\n",
    "        final_queries_image = get_image(self.final_queries_images, index)\n",
    "        \n",
    "        return self.transform(reference_image). self.transform(final_queries_image) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_feats = []\n",
    "\n",
    "device = torch.device('cuda:4')\n",
    "model.to(device)\n",
    "\n",
    "for img in datamodule.references_dataloader():\n",
    "    img = img.to(device)\n",
    "    feats = model(img)\n",
    "    ref_feats.append(feats)\n",
    "ref_feats = torch.vstack(ref_feats).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    }
   ],
   "source": [
    "query_feats = []\n",
    "\n",
    "for img in datamodule.final_queries_dataloader():\n",
    "    feats = model(img.to(device))\n",
    "    query_feats.append(feats)\n",
    "    \n",
    "query_feats = torch.vstack(query_feats).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_file_ids = lambda x: [f for f in os.listdir(x)]\n",
    "ref_ids = get_file_ids(get_path('references/'))[:100]\n",
    "query_ids = get_file_ids(get_path('final_queries'))[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import search_with_capped_res\n",
    "\n",
    "lims, dis, ids = search_with_capped_res(query_feats, ref_feats, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    2,   10,   37,   53,   53,   57,   59,   75,   75,\n",
       "         75,  103,  136,  137,  150,  170,  172,  194,  195,  207,  207,\n",
       "        207,  207,  231,  236,  248,  265,  280,  281,  292,  297,  314,\n",
       "        335,  341,  361,  362,  367,  402,  419,  421,  421,  433,  433,\n",
       "        456,  481,  481,  495,  495,  495,  497,  521,  541,  541,  541,\n",
       "        567,  572,  590,  590,  604,  604,  604,  634,  653,  653,  655,\n",
       "        655,  656,  662,  662,  689,  701,  702,  702,  717,  723,  723,\n",
       "        734,  739,  744,  754,  771,  805,  839,  861,  864,  870,  873,\n",
       "        873,  905,  921,  923,  953,  970,  976,  978,  985,  995,  996,\n",
       "        996, 1000])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([66, 87,  0, 23, 26, 28, 29, 57, 62, 91,  0,  3,  4,  6, 14, 19, 20,\n",
       "       26, 28, 32, 33, 35, 36, 43, 45, 48, 49, 57, 58, 76, 79, 81, 84, 90,\n",
       "       91, 94, 98,  3,  6, 16, 20, 26, 33, 36, 43, 55, 57, 58, 75, 81, 84,\n",
       "       90, 91, 29, 57, 74, 91, 30, 82,  0,  4, 12, 13, 19, 24, 29, 36, 49,\n",
       "       57, 69, 89, 90, 91, 94, 96,  0,  3,  4,  6, 13, 14, 19, 20, 28, 29,\n",
       "       32, 36, 43, 45, 49, 55, 56, 57, 58, 60, 62, 79, 81, 84, 90, 91, 94,\n",
       "       96,  0,  3,  4,  6,  8, 13, 14, 19, 20, 22, 29, 32, 33, 35, 36, 40,\n",
       "       43, 49, 56, 57, 58, 61, 64, 67, 76, 79, 81, 84, 89, 90, 91, 94, 96,\n",
       "       17, 10, 13, 19, 32, 36, 38, 41, 49, 59, 69, 76, 77, 85,  0,  2,  6,\n",
       "        7,  8, 14, 22, 33, 35, 36, 43, 48, 55, 58, 65, 79, 81, 84, 90, 98,\n",
       "       12, 91,  0,  1,  3,  6,  8, 26, 28, 29, 32, 35, 36, 42, 43, 45, 48,\n",
       "       57, 58, 62, 81, 84, 90, 91, 27, 13, 19, 30, 32, 41, 49, 52, 76, 77,\n",
       "       78, 82, 85,  0,  2,  3,  4,  6,  8, 14, 16, 18, 20, 33, 35, 36, 43,\n",
       "       45, 46, 48, 54, 55, 58, 75, 79, 81, 90,  3,  6, 28, 60, 62, 12, 13,\n",
       "       19, 24, 29, 32, 49, 57, 69, 89, 91, 96,  6, 14, 19, 20, 22, 32, 36,\n",
       "       38, 43, 49, 61, 64, 67, 76, 79, 81, 84,  0,  6, 23, 26, 28, 29, 35,\n",
       "       36, 45, 57, 62, 81, 90, 91, 96, 95, 11, 30, 41, 49, 52, 66, 71, 76,\n",
       "       77, 82, 87, 28, 29, 57, 62, 63, 10, 13, 19, 21, 24, 29, 32, 36, 41,\n",
       "       49, 67, 69, 76, 77, 84, 85, 91, 25, 78, 99, 38,  4, 14, 59, 43,  5,\n",
       "       44, 32, 49, 41, 37, 22, 77, 84, 47, 79, 19, 20, 62, 28, 57, 29, 91,\n",
       "       23, 81, 58, 48, 43, 35, 36,  6, 33, 90,  3, 84, 55, 79, 45, 14,  0,\n",
       "        8, 20, 16, 98, 95, 74, 57, 91, 29, 19, 49, 84, 36, 91, 43, 32, 81,\n",
       "       58, 19, 13, 94, 76, 59, 90,  4, 29, 14, 40,  6, 20, 67, 53, 24, 34,\n",
       "       21, 89,  0, 77, 25, 69, 79, 38, 33, 41, 57, 62,  3, 42, 45, 28, 97,\n",
       "       23,  6, 81,  0, 48,  1, 57, 26, 35, 90, 55, 12, 29, 69, 76, 49, 19,\n",
       "       91, 77, 24, 13, 12, 30, 94, 32, 81,  6, 84, 36, 43,  3, 91, 90, 29,\n",
       "       49,  0, 32, 76, 58, 57, 61, 20, 75,  8, 40, 33, 56,  4, 58, 79, 33,\n",
       "        8, 55, 48, 81, 20, 90, 35, 43, 14,  2, 36, 16, 54, 84, 46, 75,  6,\n",
       "        4, 25,  3,  0, 45, 20, 90, 14, 24, 40, 94, 58, 19, 49,  4,  0, 59,\n",
       "       13, 43, 28, 35, 91, 57, 19, 90, 29, 94, 12, 36,  0, 89,  4, 13, 96,\n",
       "       24, 59, 49, 20, 34, 26, 58, 84, 53, 32,  6, 79, 58, 55, 14, 90,  8,\n",
       "       81, 43,  6, 84, 48, 33,  2, 35,  3,  0, 20, 36, 98,  4, 19, 91, 29,\n",
       "       49, 69, 94, 57, 36, 76, 32, 24, 13,  6, 60, 40, 12, 20,  4, 84, 81,\n",
       "       74, 90, 89, 96, 21, 43, 29, 91, 69, 21, 13, 85, 59, 41, 13, 19, 49,\n",
       "        4, 25, 52, 30, 77, 38, 69, 76, 10, 32,  5, 44, 29, 91, 21, 19, 24,\n",
       "       49, 13, 89, 96, 57, 92, 76, 10, 36, 43, 81, 79, 84, 14, 58,  8, 33,\n",
       "       48, 64, 36,  2, 55,  7, 32, 67, 22, 35, 49, 25, 90, 20, 65, 99,  4,\n",
       "        6, 38,  3,  0, 98, 35, 48, 58, 54, 79, 33, 81, 90, 55,  2,  8, 98,\n",
       "       43,  0,  6, 36, 31, 14, 20, 30, 87, 74, 57, 74, 29, 91, 69, 62, 59,\n",
       "       41,  4, 25, 78, 49, 19, 13, 32,  5, 38, 44, 43, 83, 36, 30, 85, 14,\n",
       "       99, 20, 76, 84, 52, 77, 69, 67, 47, 29,  0, 36, 81, 62, 96, 91, 89,\n",
       "       58, 90, 57, 32, 54, 84, 49, 81, 76, 32, 43, 64, 36, 67, 90,  0, 61,\n",
       "       58,  6,  4, 28, 62, 29, 57,  6, 23, 12, 30, 69, 24, 13, 40, 94, 52,\n",
       "       59, 50, 89, 62, 28,  0, 29, 57, 12,  4, 89, 50, 96, 77, 49, 76, 32,\n",
       "       85, 41, 10, 80, 38, 19, 28, 56, 62, 29,  6, 57, 60, 91, 96, 81, 90,\n",
       "        0, 36,  3, 89, 61, 84, 32, 49, 36, 84, 81, 43, 91, 29, 13,  0,  4,\n",
       "       67, 90, 19, 57, 58, 14, 20, 21, 26, 76, 59,  6, 94, 89, 25, 64, 33,\n",
       "       53, 10, 96, 79, 69, 44, 36, 81, 90,  4,  0, 29, 20, 58, 43, 89, 84,\n",
       "        6, 19, 91, 33, 62, 57, 32, 79, 94, 53, 45,  8, 35, 49, 14, 56, 16,\n",
       "       96, 40, 76, 46, 59, 48, 29, 91, 12, 57, 96, 69, 19, 60, 89, 56, 62,\n",
       "       13, 24,  6, 74, 50, 94, 40, 36, 90,  0, 49, 28, 62, 42, 78, 25, 99,\n",
       "       41,  7, 38, 62, 28, 29, 33, 58,  4, 43, 14, 90,  2, 37, 98, 54, 79,\n",
       "        0, 20, 81, 36, 25, 35,  8, 84, 78, 22,  7, 49, 48, 44, 65, 34, 59,\n",
       "       47, 19,  6, 99, 49, 19, 91, 69, 13, 76, 29, 60, 24, 12, 40, 32, 59,\n",
       "        4, 36, 94, 50, 69, 91, 19, 94, 13, 49, 24, 40, 34, 12, 36, 15, 59,\n",
       "       89, 32, 29, 69, 76,  4, 90, 58, 84, 21, 53, 20, 96, 81, 43, 57, 77,\n",
       "        0, 57, 62, 28, 26, 91, 90,  6,  0, 23, 36, 81, 58, 29, 45, 35,  3,\n",
       "       94, 59,  4, 37, 15,  5, 78, 18, 46, 41, 30, 87, 71, 77, 76, 49, 62,\n",
       "       28, 57, 29, 56, 23, 26,  0,  6, 42, 23, 32, 95, 84, 49])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_list = []\n",
    "for i in range(100):\n",
    "    for j in range(lims[i], lims[i+1]):\n",
    "        #predictions_list.append((lims[i], ids[j]))\n",
    "        predictions_list.append([query_ids[i], ref_ids[ids[j]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "copydetectpred = CopyDetectPredDataset(predictions = predictions_list,\n",
    "                                       references_dir = get_path('references'),\n",
    "                                       final_queries_dir = get_path('final_queries/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "copydetect_dataloader = DataLoader(dataset = copydetectpred,\n",
    "                                   batch_size = 16,\n",
    "                                   num_workers = 8,\n",
    "                                   pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Caught AttributeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_29059/4066742790.py\", line 26, in __getitem__\n    return self.transform(reference_image). self.transform(final_queries_image)\nAttributeError: 'Tensor' object has no attribute 'self'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/leejiahe/copydetection/model.ipynb Cell 18'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bidsd-1.d2.comp.nus.edu.sg/home/leejiahe/copydetection/model.ipynb#ch0000006vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mnext\u001b[39;49m(\u001b[39miter\u001b[39;49m(copydetect_dataloader))\n",
      "File \u001b[0;32m~/anaconda3/envs/cama/lib/python3.8/site-packages/torch/utils/data/dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=518'>519</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=519'>520</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=520'>521</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=521'>522</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=522'>523</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=523'>524</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=524'>525</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/cama/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1203\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=1200'>1201</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=1201'>1202</a>\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=1202'>1203</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
      "File \u001b[0;32m~/anaconda3/envs/cama/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1229\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=1226'>1227</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=1227'>1228</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=1228'>1229</a>\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m   <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=1229'>1230</a>\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/cama/lib/python3.8/site-packages/torch/_utils.py:425\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/_utils.py?line=420'>421</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexc_type, \u001b[39m\"\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/_utils.py?line=421'>422</a>\u001b[0m     \u001b[39m# Some exceptions have first argument as non-str but explicitly\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/_utils.py?line=422'>423</a>\u001b[0m     \u001b[39m# have message field\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/_utils.py?line=423'>424</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexc_type(message\u001b[39m=\u001b[39mmsg)\n\u001b[0;32m--> <a href='file:///home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/_utils.py?line=424'>425</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexc_type(msg)\n",
      "\u001b[0;31mAttributeError\u001b[0m: Caught AttributeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/leejiahe/anaconda3/envs/cama/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_29059/4066742790.py\", line 26, in __getitem__\n    return self.transform(reference_image). self.transform(final_queries_image)\nAttributeError: 'Tensor' object has no attribute 'self'\n"
     ]
    }
   ],
   "source": [
    "next(iter(copydetect_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "496e6ccc940d7baf27126816b5de78ce76f9b9d17be85a2d2d9dfc63004cf58f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('cama')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
