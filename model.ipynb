{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import einops\n",
    "from PIL import Image\n",
    "from typing import Any, List, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from torchmetrics import MaxMetric\n",
    "from torchmetrics.classification.accuracy import Accuracy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from transformers import ViTModel\n",
    "\n",
    "from src.models.components.layers import CopyDetectEmbedding, NormalizedFeatures, SimImagePred, ContrastiveProj\n",
    "from src.datamodules.copydetect_datamodule import CopyDetectDataModule\n",
    "from src.datamodules.components.augmentation import Augment\n",
    "from src.utils.nt_xent_loss import NTXentLoss\n",
    "\n",
    "\n",
    "get_path = lambda x: os.path.join(os.getcwd(),'data', x)\n",
    "\n",
    "augment = Augment(overlay_image_dir = get_path('train/'),\n",
    "                  n_upper = 2,\n",
    "                  n_lower = 1)\n",
    "\n",
    "ntxentloss = NTXentLoss(temperature = 0.9, eps = 1e-5)\n",
    "\n",
    "device = torch.device('cuda:4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CopyDetectModule(LightningModule):\n",
    "    def __init__(self,\n",
    "                 pretrained_arch: str,          # Pretrained ViT architecture\n",
    "                 ntxentloss: object,            # Contrastive loss\n",
    "                 hidden_dim: int = 2048,        # Contrastive projection size of hidden layer\n",
    "                 projected_dim: int = 512,      # Contrastive projection size of projection head \n",
    "                 beta1: int = 1,                # Similar image BCE loss multiplier\n",
    "                 beta2: int = 1,                # Contrastive loss multiplier\n",
    "                 lr: float = 0.001,\n",
    "                 weight_decay: float = 0.0005):               \n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(logger = False)\n",
    "         \n",
    "        # Instantiate ViT encoder from pretrained model\n",
    "        pretrained_model = ViTModel.from_pretrained(pretrained_arch)\n",
    "        encoder = pretrained_model.encoder\n",
    "        self.patch_size = pretrained_model.config.patch_size\n",
    "                \n",
    "        # Instantiate embedding, we use the pretrained ViT cls and position embedding\n",
    "        embedding = CopyDetectEmbedding(config = pretrained_model.config,\n",
    "                                        vit_cls = pretrained_model.embeddings.cls_token,\n",
    "                                        pos_emb = pretrained_model.embeddings.position_embeddings)\n",
    "        \n",
    "        # Normalized features\n",
    "        normfeats = NormalizedFeatures(hidden_dim = pretrained_model.config.hidden_size,\n",
    "                                       layer_norm_eps = pretrained_model.config.layer_norm_eps)\n",
    "        # Feature Vector Extractor\n",
    "        self.feature_extractor = nn.Sequential(embedding, encoder, normfeats)\n",
    "        \n",
    "        # Instantiate SimImagePredictor\n",
    "        simimagepred = SimImagePred(embedding_dim = pretrained_model.config.hidden_size)\n",
    "        self.embedding = embedding\n",
    "        self.simimagepred = nn.Sequential(encoder, normfeats, simimagepred)\n",
    "\n",
    "        # Instantiate ContrastiveProjection\n",
    "        contrastiveproj = ContrastiveProj(embedding_dim = pretrained_model.config.hidden_size,\n",
    "                                          hidden_dim = hidden_dim,\n",
    "                                          projected_dim = projected_dim)\n",
    "        self.contrastiveproj = nn.Sequential(embedding, encoder, normfeats, contrastiveproj)\n",
    "        \n",
    "        # Contrastive loss \n",
    "        self.contrastive_loss = ntxentloss\n",
    "        \n",
    "        # Binary cross entropy loss for similar image pair\n",
    "        self.bce_loss = torch.nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        # Model accuracy in detecting modified copy\n",
    "        self.train_acc, self.val_acc = Accuracy(), Accuracy()   \n",
    "          \n",
    "        # For logging best validation accuracy\n",
    "        self.val_acc_best = MaxMetric()\n",
    "\n",
    "    def feature_extract(self, batch: Any) -> torch.Tensor:\n",
    "        # To extract feature vector\n",
    "        img_r, img_id = batch\n",
    "        encoding = self.feature_extractor(img_r)\n",
    "        batch_size, num_ch, H, W, = img_r.size()\n",
    "        #dim = encoding.size(2) # batch_size, seq_len, dim \n",
    "        h, w = int(H/self.patch_size), int(W/self.patch_size)\n",
    "        cls, feats = encoding[:,0,:], encoding[:,1:,:] # Get the cls token and all the images features\n",
    "            \n",
    "        #feats = feats.reshape(batch_size, h, w, dim).clamp(min = 1e-6).permute(0,3,1,2)\n",
    "        feats = einops.rearrange(feats, 'b (h w) d -> b d h w', h = h, w = w).clamp(min = 1e-6)\n",
    "        # GeM Pooling\n",
    "        feats = F.avg_pool2d(feats.pow(4), (h,w)).pow(1./4)\n",
    "        feats = einops.rearrange(feats, 'b d () () -> b d')\n",
    "        # Concatenate cls tokens with image patches to give local and global views of image\n",
    "        feature_vector = torch.cat((cls, feats), dim = 1)\n",
    "\n",
    "        return feature_vector, img_id\n",
    "        \n",
    "    def predict_copy(self, batch):\n",
    "        # For copy detection \n",
    "        img_r, img_q = batch\n",
    "        embedding_rq = self.embedding(img_r, img_q)\n",
    "        logits = self.simimagepred(embedding_rq)\n",
    "        preds = torch.argmax(logits, dim = 1)\n",
    "        return preds\n",
    "\n",
    "    def step(self,\n",
    "             img_r: List[torch.Tensor],\n",
    "             img_q: List[torch.Tensor],\n",
    "             label: List[torch.Tensor]):\n",
    "        \n",
    "        # img_r, img_q to SimImagePredictor\n",
    "        embedding_rq = self.embedding(img_r, img_q) ## nn sequential don't take multiple input\n",
    "        logits = self.simimagepred(embedding_rq)\n",
    "        # Calculate binary cross entropy loss of similar image pair\n",
    "        simimage_loss = self.bce_loss(logits, label.unsqueeze(dim = 1))\n",
    "        # Predictions\n",
    "        preds = torch.argmax(logits, dim = 1)\n",
    "        \n",
    "        # Get positive indices\n",
    "        pos_indices = label.bool()\n",
    "        # Forward positive indices of img_r and img_q to ContrastiveProjection\n",
    "        proj_r = self.contrastiveproj(img_r[pos_indices])\n",
    "        proj_q = self.contrastiveproj(img_q[pos_indices])\n",
    "\n",
    "        # Calculate contrastive loss between un-augmented img_r and augmented positive pair of img_q\n",
    "        contrastive_loss = self.contrastive_loss(proj_r, proj_q)\n",
    "        \n",
    "        # Weighted sum of bce and contrastive loss\n",
    "        total_loss = self.hparams.beta1 * simimage_loss + self.hparams.beta2 * contrastive_loss\n",
    "        \n",
    "        return {'simimage': simimage_loss, 'contrastive': contrastive_loss, 'total': total_loss}, preds\n",
    "\n",
    "    def training_step(self, batch: Any, batch_idx: int):\n",
    "        img_r, img_q, label = batch\n",
    "        img_r, img_q, label = torch.vstack(img_r), torch.vstack(img_q), torch.hstack(label)\n",
    "\n",
    "        losses, preds = self.step(img_r, img_q, label)\n",
    "        \n",
    "        # Log train metrics\n",
    "        acc = self.train_acc(preds, label.int())\n",
    "        self.log(\"train/total_loss\", losses['total'], on_step = True, on_epoch = True, prog_bar = False)\n",
    "        self.log(\"train/simimage_loss\", losses['simimage'], on_step = True, on_epoch = True, prog_bar = False)\n",
    "        self.log(\"train/contrastive_loss\", losses['contrastive'], on_step = True, on_epoch = True, prog_bar = False)\n",
    "        self.log(\"train/acc\", acc, on_step = True, on_epoch = True, prog_bar = True)\n",
    "\n",
    "        return losses['total']\n",
    "\n",
    "    def validation_step(self, batch: Any, batch_idx: int):\n",
    "        img_r, img_q, label = batch\n",
    "        losses, preds = self.step(img_r, img_q, label)\n",
    "\n",
    "        # Log val metrics\n",
    "        acc = self.val_acc(preds, label.int())\n",
    "        self.log(\"val/total_loss\", losses['total'], on_step = True, on_epoch = True, prog_bar = False)\n",
    "        self.log(\"val/simimage_loss\", losses['simimage'], on_step = True, on_epoch = True, prog_bar = False)\n",
    "        self.log(\"val/contrastive_loss\", losses['contrastive'], on_step = True, on_epoch = True, prog_bar = False)\n",
    "        self.log(\"val/acc\", acc, on_step = True, on_epoch = True, prog_bar = True)\n",
    "\n",
    "        return losses['total']\n",
    "\n",
    "    def validation_epoch_end(self, outputs: Any):\n",
    "        acc = self.val_acc.compute()  # get val accuracy from current epoch\n",
    "        self.val_acc_best.update(acc)\n",
    "        self.log(\"val/acc_best\", self.val_acc_best.compute(), on_epoch = True, prog_bar = True)\n",
    "        \n",
    "    def test_step(self, batch: Any, batch_idx: int):\n",
    "        feats = self.feature_extract(batch) # Get feat\n",
    "        \n",
    "        return feats\n",
    "    \n",
    "    def test_epoch_end(self, test_step_outputs: Any):\n",
    "        all_feats, all_ids = [], []\n",
    "        for step_output in test_step_outputs:\n",
    "            all_feats.append(step_output[0])\n",
    "            all_ids.extend(step_output[1])\n",
    "            \n",
    "        all_feats = torch.vstack(all_feats)\n",
    "        self.test_results = (all_feats, all_ids)\n",
    "        \n",
    "        return all_feats\n",
    "    \n",
    "    def predict_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0):\n",
    "        self.test_results = None\n",
    "        score = self.predict_copy(batch)\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        # Reset metrics at the end of every epoch\n",
    "        self.train_acc.reset()\n",
    "        self.val_acc.reset()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(params = self.parameters(),\n",
    "                                lr = self.hparams.lr,\n",
    "                                weight_decay = self.hparams.weight_decay)\n",
    "        \n",
    "pretrained_arch = 'google/vit-base-patch16-224'\n",
    "\n",
    "model = CopyDetectModule(pretrained_arch, ntxentloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = CopyDetectDataModule(train_dir = get_path('train/'),\n",
    "                                  references_dir = get_path('references/'),\n",
    "                                  dev_queries_dir = get_path('dev_queries/'),\n",
    "                                  final_queries_dir = get_path('final_queries/'),\n",
    "                                  augment = augment,\n",
    "                                  dev_validation_set = get_path('dev_validation_set.csv'),\n",
    "                                  batch_size = 16,\n",
    "                                  pin_memory = True,\n",
    "                                  num_workers = 10,\n",
    "                                  n_crops = 2)\n",
    "\n",
    "datamodule.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(accelerator = 'gpu', devices = [4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model = model, dataloaders = datamodule.references_dataloader())\n",
    "r, r_id = model.test_results\n",
    "r = r.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model = model, dataloaders = datamodule.final_queries_dataloader())\n",
    "q, q_id = model.test_results\n",
    "q = q.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_SDEV = [0.229, 0.224, 0.225]\n",
    "\n",
    "transform =  transforms.Compose([transforms.ToTensor(),\n",
    "                                 transforms.Resize((224, 224)),\n",
    "                                 transforms.Normalize(IMAGENET_MEAN, IMAGENET_SDEV)])\n",
    "\n",
    "get_image = lambda img_dir, img: Image.open(os.path.join(img_dir, img))\n",
    "get_image_file = lambda image_dir: [os.path.join(image_dir, f) for f in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, f))]\n",
    "\n",
    "class CopyDetectPredDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 predictions: list,\n",
    "                 references_dir: str,\n",
    "                 final_queries_dir: str):\n",
    "        self.predictions = predictions\n",
    "        self.references_dir = references_dir\n",
    "        self.final_queries_dir = final_queries_dir\n",
    "        self.transform =  transforms.Compose([transforms.ToTensor(),\n",
    "                                              transforms.Resize((224, 224)),\n",
    "                                              transforms.Normalize(IMAGENET_MEAN, IMAGENET_SDEV)])\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.predictions)\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        final_queries_id, references_id = self.predictions[index][0], self.predictions[index][1]\n",
    "        \n",
    "        reference_image = get_image(self.references_dir, references_id)\n",
    "        final_queries_image = get_image(self.final_queries_dir, final_queries_id)\n",
    "        \n",
    "        return self.transform(reference_image), self.transform(final_queries_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import search_with_capped_res\n",
    "\n",
    "lims, dis, ids = search_with_capped_res(q, r, 1000)\n",
    "\n",
    "predictions_list = []\n",
    "for i in range(100):\n",
    "    for j in range(lims[i], lims[i+1]):\n",
    "        predictions_list.append([q_id[i], r_id[ids[j]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copydetectpred = CopyDetectPredDataset(predictions = predictions_list,\n",
    "                                       references_dir = get_path('references'),\n",
    "                                       final_queries_dir = get_path('final_queries/'))\n",
    "\n",
    "\n",
    "copydetect_dataloader = DataLoader(dataset = copydetectpred,\n",
    "                                   batch_size = 16,\n",
    "                                   num_workers = 8,\n",
    "                                   pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = trainer.predict(model = model, dataloaders = copydetect_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.hstack(p).cpu().numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_feats, ref_ids = [], []\n",
    "\n",
    "for batch in datamodule.references_dataloader():\n",
    "    feats, img_id = model.feature_extract(batch)\n",
    "    ref_feats.append(feats)\n",
    "    ref_ids.extend(img_id)\n",
    "ref_feats = torch.vstack(ref_feats).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_feats, query_ids = [], []\n",
    "\n",
    "for batch in datamodule.final_queries_dataloader():\n",
    "    img, img_id = batch\n",
    "    feats = model(img.to(device))\n",
    "    query_feats.append(feats)\n",
    "    query_ids.extend(img_id)\n",
    "    \n",
    "query_feats = torch.vstack(query_feats).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import search_with_capped_res\n",
    "\n",
    "lims, dis, ids = search_with_capped_res(query_feats, ref_feats, 1000)\n",
    "\n",
    "predictions_list = []\n",
    "for i in range(100):\n",
    "    for j in range(lims[i], lims[i+1]):\n",
    "        predictions_list.append([query_ids[i], ref_ids[ids[j]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.test_results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for searches in copydetect_dataloader:\n",
    "    query_id, ref_id = searches\n",
    "    query_id, ref_id = query_id.to(device), ref_id.to(device)\n",
    "    score = model(ref_id, query_id)\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = torch.hstack(scores).detach().cpu().numpy()\n",
    "scores.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class CopyDetectPretrainDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 image_dir: str):\n",
    "        \n",
    "        self.image_files = np.array([os.path.join(image_dir, f) \n",
    "                                     for f in os.listdir(image_dir) \n",
    "                                     if os.path.isfile(os.path.join(image_dir, f))])\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "            image = Image.open(self.image_files[index])\n",
    "            return image\n",
    "        \n",
    "train_dataset = CopyDetectPretrainDataset(image_dir = get_path('references/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datamodules.components.augmentation import Augment\n",
    "\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_SDEV = [0.229, 0.224, 0.225]\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Resize((224, 224)),\n",
    "                                transforms.Normalize(IMAGENET_MEAN, IMAGENET_SDEV)])\n",
    "\n",
    "augment = Augment(overlay_image_dir = get_path('references/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CopyDetectCollateFn(nn.Module):\n",
    "    def __init__(self,\n",
    "                 transform,\n",
    "                 augment: object,\n",
    "                 n_crops: Optional[int] = 1):\n",
    "        super().__init__()\n",
    "        self.transform  = transform\n",
    "        self.augment = augment\n",
    "        self.n_crops = n_crops\n",
    "\n",
    "    def forward(self, batch):\n",
    "        batch_size = len(batch)\n",
    "        indices = np.arange(batch_size)\n",
    "        \n",
    "        imgs = list(map(lambda x: self.transform(x).unsqueeze_(dim = 0), batch))\n",
    "        imgs_list, aug_list, label_list = [], [], []\n",
    "        for _ in range(self.n_crops):\n",
    "            rand_bool = np.random.uniform(size = batch_size) < 0.5\n",
    "            rand_indices = np.random.randint(0, batch_size, size = batch_size)\n",
    "            aug_indices = np.where(rand_bool, indices, rand_indices)\n",
    "            aug_imgs = list(map(lambda i: batch[i], aug_indices))\n",
    "            aug_imgs = list(map(lambda x: self.transform(self.augment(x)).unsqueeze_(dim = 0), aug_imgs))\n",
    "            \n",
    "            label = torch.tensor(indices == rand_indices, dtype = torch.float) # label 1: modified copy: aug_index == index \n",
    "\n",
    "            imgs_list.extend(imgs)\n",
    "            aug_list.extend(aug_imgs)\n",
    "            label_list.extend(label)\n",
    "            \n",
    "        \n",
    "        imgs_list = torch.vstack(imgs_list)\n",
    "        aug_list = torch.vstack(aug_list)\n",
    "        label_list = torch.hstack(label_list)\n",
    "        #return imgs_list, aug_list, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = CopyDetectCollateFn(transform = transform,\n",
    "                                 augment = augment,\n",
    "                                 n_crops = 4)\n",
    "\n",
    "dataloader = DataLoader(dataset = train_dataset,\n",
    "                        collate_fn = collate_fn,\n",
    "                        batch_size = 8)\n",
    "\n",
    "next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ['/home/leejiahe/copydetection/data/references/R056915.jpg', '/home/leejiahe/copydetection/data/references/R565046.jpg', '/home/leejiahe/copydetection/data/references/R694596.jpg', '/home/leejiahe/copydetection/data/references/R971542.jpg', '/home/leejiahe/copydetection/data/references/R193894.jpg', '/home/leejiahe/copydetection/data/references/R933667.jpg', '/home/leejiahe/copydetection/data/references/R685573.jpg', '/home/leejiahe/copydetection/data/references/R664814.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = ['/home/leejiahe/copydetection/data/references/R056915.jpg', '/home/leejiahe/copydetection/data/references/R326673.jpg', '/home/leejiahe/copydetection/data/references/R694596.jpg', '/home/leejiahe/copydetection/data/references/R971542.jpg', '/home/leejiahe/copydetection/data/references/R193894.jpg', '/home/leejiahe/copydetection/data/references/R344586.jpg', '/home/leejiahe/copydetection/data/references/R685573.jpg', '/home/leejiahe/copydetection/data/references/R772966.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i == j for i, j in zip(a, b)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(map(lambda x: transform(Image.open(x)), a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "indices = np.arange(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False,  True, False, False,  True, False,\n",
       "        True, False,  True,  True,  True, False,  True])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_bool = np.random.uniform(size = batch_size) < 0.5\n",
    "rand_bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10,  7, 12,  4,  9, 14, 12,  2,  1,  8,  4,  9,  4,  3,  4,  1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_indices = np.random.randint(0, batch_size, size = batch_size)\n",
    "rand_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10,  7, 12,  4,  4, 14, 12,  7,  1,  9,  4, 11, 12, 13,  4, 15])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_indices = np.where(rand_bool, indices, rand_indices)\n",
    "shuffled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 7, 12, 4, 4, 14, 12, 7, 1, 9, 4, 11, 12, 13, 4, 15]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_indices.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = indices == shuffled_indices\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5271, 0.6336, 0.6405],\n",
       "        [0.8281, 0.2890, 0.0345],\n",
       "        [0.1625, 0.6298, 0.3686],\n",
       "        [0.6862, 0.6554, 0.3663],\n",
       "        [0.8792, 0.1040, 0.8082],\n",
       "        [0.8966, 0.6050, 0.9391],\n",
       "        [0.1756, 0.5772, 0.2481],\n",
       "        [0.8984, 0.5844, 0.2743],\n",
       "        [0.4769, 0.5165, 0.9868],\n",
       "        [0.5686, 0.6177, 0.0703],\n",
       "        [0.3386, 0.5637, 0.6735],\n",
       "        [0.9954, 0.5684, 0.1002],\n",
       "        [0.1760, 0.8427, 0.6575],\n",
       "        [0.0631, 0.0117, 0.2871],\n",
       "        [0.4041, 0.9815, 0.6798],\n",
       "        [0.2629, 0.6978, 0.3208]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "z = torch.rand(size = (batch_size, 3))\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3386, 0.5637, 0.6735],\n",
       "        [0.8984, 0.5844, 0.2743],\n",
       "        [0.1760, 0.8427, 0.6575],\n",
       "        [0.8792, 0.1040, 0.8082],\n",
       "        [0.8792, 0.1040, 0.8082],\n",
       "        [0.4041, 0.9815, 0.6798],\n",
       "        [0.1760, 0.8427, 0.6575],\n",
       "        [0.8984, 0.5844, 0.2743],\n",
       "        [0.8281, 0.2890, 0.0345],\n",
       "        [0.5686, 0.6177, 0.0703],\n",
       "        [0.8792, 0.1040, 0.8082],\n",
       "        [0.9954, 0.5684, 0.1002],\n",
       "        [0.1760, 0.8427, 0.6575],\n",
       "        [0.0631, 0.0117, 0.2871],\n",
       "        [0.8792, 0.1040, 0.8082],\n",
       "        [0.2629, 0.6978, 0.3208]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z[shuffled_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "496e6ccc940d7baf27126816b5de78ce76f9b9d17be85a2d2d9dfc63004cf58f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('cama')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
